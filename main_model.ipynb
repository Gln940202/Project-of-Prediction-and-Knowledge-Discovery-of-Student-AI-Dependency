{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "# 中文注释：基础库与路径设置；字体、随机种子、输出目录\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'  # 全局 Times\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 建模与评估\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# XGBoost（如未安装，请先：pip install xgboost shap openpyxl）\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 缺失值插补\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# 持久化\n",
    "import joblib\n",
    "\n",
    "# 路径\n",
    "OUT_DIR = \"outputs\"\n",
    "DES_DIR = os.path.join(OUT_DIR, \"des\")\n",
    "FIG_DIR = os.path.join(OUT_DIR, \"figs\")\n",
    "MODEL_DIR = os.path.join(OUT_DIR, \"models\")\n",
    "LOG_DIR = os.path.join(OUT_DIR, \"logs\")\n",
    "\n",
    "for d in [OUT_DIR, DES_DIR, FIG_DIR, MODEL_DIR, LOG_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "SEED = 20251008\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一部分：数据读取和展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (860, 50)\n",
      "Columns (head): ['school_type', 'school_type_encode', 'gender', 'gender_encode', 'grade_level', 'grade_level_encode', 'major_cat', 'major_cat_encode', 'hukou_type', 'hukou_type_encode', 'age', 'parent_edu_f', 'parent_edu_m', 'family_income_quintile', 'ai_use_days_per_week', 'ai_daily_minutes', 'ai_tool_diversity', 'prompt_length_avg', 'iter_per_task', 'edit_ratio_self']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load data\n",
    "# 中文注释：载入 data.xlsx；自动识别目标变量与可能的字符串分类列\n",
    "DATA_PATH = \"data.xlsx\"  \n",
    "\n",
    "df_raw = pd.read_excel(DATA_PATH)\n",
    "\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "print(\"Columns (head):\", list(df_raw.columns)[:20])\n",
    "\n",
    "TARGET = \"ai_dep_dai_total\"\n",
    "assert TARGET in df_raw.columns, f\"Target '{TARGET}' not found!\"\n",
    "\n",
    "# 可能的字符串分类列（若存在就统计与画图）\n",
    "CATEGORICAL_STR_COLS = [\n",
    "    \"school_type\",\"gender\", \"grade_level\", \"major_cat\",\n",
    "    \"hukou_type\", \"teacher_ai_guidance\", \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_type</th>\n",
       "      <th>school_type_encode</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender_encode</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>grade_level_encode</th>\n",
       "      <th>major_cat</th>\n",
       "      <th>major_cat_encode</th>\n",
       "      <th>hukou_type</th>\n",
       "      <th>hukou_type_encode</th>\n",
       "      <th>...</th>\n",
       "      <th>peer_ai_norms</th>\n",
       "      <th>course_difficulty</th>\n",
       "      <th>ai_paid_sub</th>\n",
       "      <th>dai_item_1</th>\n",
       "      <th>dai_item_2</th>\n",
       "      <th>dai_item_3</th>\n",
       "      <th>dai_item_4</th>\n",
       "      <th>dai_item_5</th>\n",
       "      <th>dai_item_6</th>\n",
       "      <th>ai_dep_dai_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>专科</td>\n",
       "      <td>0</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大四</td>\n",
       "      <td>4</td>\n",
       "      <td>文学类</td>\n",
       "      <td>10</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>男</td>\n",
       "      <td>0</td>\n",
       "      <td>大一</td>\n",
       "      <td>1</td>\n",
       "      <td>计算机类</td>\n",
       "      <td>0</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大一</td>\n",
       "      <td>1</td>\n",
       "      <td>化学类</td>\n",
       "      <td>5</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大一</td>\n",
       "      <td>1</td>\n",
       "      <td>化学类</td>\n",
       "      <td>5</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大二</td>\n",
       "      <td>2</td>\n",
       "      <td>思想政治教育类</td>\n",
       "      <td>12</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>专科</td>\n",
       "      <td>0</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大四</td>\n",
       "      <td>4</td>\n",
       "      <td>自动化类</td>\n",
       "      <td>2</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>双一流</td>\n",
       "      <td>2</td>\n",
       "      <td>男</td>\n",
       "      <td>0</td>\n",
       "      <td>大三</td>\n",
       "      <td>3</td>\n",
       "      <td>教育学类</td>\n",
       "      <td>15</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大一</td>\n",
       "      <td>1</td>\n",
       "      <td>新闻传播学类</td>\n",
       "      <td>11</td>\n",
       "      <td>城镇</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>双一流</td>\n",
       "      <td>2</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大二</td>\n",
       "      <td>2</td>\n",
       "      <td>计算机类</td>\n",
       "      <td>0</td>\n",
       "      <td>城镇</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>男</td>\n",
       "      <td>0</td>\n",
       "      <td>大三</td>\n",
       "      <td>3</td>\n",
       "      <td>体育学类</td>\n",
       "      <td>13</td>\n",
       "      <td>城镇</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school_type  school_type_encode gender  gender_encode grade_level  \\\n",
       "0            专科                   0      女              1          大四   \n",
       "1          普通本科                   1      男              0          大一   \n",
       "2          普通本科                   1      女              1          大一   \n",
       "3          普通本科                   1      女              1          大一   \n",
       "4          普通本科                   1      女              1          大二   \n",
       "..          ...                 ...    ...            ...         ...   \n",
       "855          专科                   0      女              1          大四   \n",
       "856         双一流                   2      男              0          大三   \n",
       "857        普通本科                   1      女              1          大一   \n",
       "858         双一流                   2      女              1          大二   \n",
       "859        普通本科                   1      男              0          大三   \n",
       "\n",
       "     grade_level_encode major_cat  major_cat_encode hukou_type  \\\n",
       "0                     4       文学类                10         农村   \n",
       "1                     1      计算机类                 0         农村   \n",
       "2                     1       化学类                 5         农村   \n",
       "3                     1       化学类                 5         农村   \n",
       "4                     2   思想政治教育类                12         农村   \n",
       "..                  ...       ...               ...        ...   \n",
       "855                   4      自动化类                 2         农村   \n",
       "856                   3      教育学类                15         农村   \n",
       "857                   1    新闻传播学类                11         城镇   \n",
       "858                   2      计算机类                 0         城镇   \n",
       "859                   3      体育学类                13         城镇   \n",
       "\n",
       "     hukou_type_encode  ...  peer_ai_norms  course_difficulty  ai_paid_sub  \\\n",
       "0                    0  ...              1                  4            0   \n",
       "1                    0  ...              3                  2            0   \n",
       "2                    0  ...              1                  1            0   \n",
       "3                    0  ...              5                  3            1   \n",
       "4                    0  ...              3                  3            0   \n",
       "..                 ...  ...            ...                ...          ...   \n",
       "855                  0  ...              5                  4            0   \n",
       "856                  0  ...              1                  2            1   \n",
       "857                  1  ...              3                  3            0   \n",
       "858                  1  ...              5                  4            1   \n",
       "859                  1  ...              3                  4            0   \n",
       "\n",
       "     dai_item_1  dai_item_2  dai_item_3  dai_item_4  dai_item_5  dai_item_6  \\\n",
       "0             1           0           0           1           0           0   \n",
       "1             4           5           3           1           4           1   \n",
       "2             3           3           4           1           1           5   \n",
       "3             4           4           1           4           4           2   \n",
       "4             5           1           3           0           1           0   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "855           2           5           1           2           0           1   \n",
       "856           3           0           5           3           1           2   \n",
       "857           3           0           2           3           1           2   \n",
       "858           0           4           1           4           4           3   \n",
       "859           3           2           4           3           5           2   \n",
       "\n",
       "     ai_dep_dai_total  \n",
       "0                   2  \n",
       "1                  18  \n",
       "2                  17  \n",
       "3                  19  \n",
       "4                  10  \n",
       "..                ...  \n",
       "855                11  \n",
       "856                14  \n",
       "857                11  \n",
       "858                16  \n",
       "859                19  \n",
       "\n",
       "[860 rows x 50 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分： 数据探索分析和描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs\\des\\continuous_descriptive.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_cols = [c for c in df_raw.columns if pd.api.types.is_numeric_dtype(df_raw[c])]\n",
    "num_cols = [c for c in num_cols if c.lower() not in [\"id\", \"valid_flag\"]]  # 排除ID等\n",
    "desc = pd.DataFrame({\n",
    "    \"mean\": df_raw[num_cols].mean(),\n",
    "    \"std\": df_raw[num_cols].std(),\n",
    "    \"min\": df_raw[num_cols].min(),\n",
    "    \"median\": df_raw[num_cols].median(),\n",
    "    \"max\": df_raw[num_cols].max()\n",
    "})\n",
    "desc = desc.loc[:, [\"mean\", \"std\", \"min\", \"median\", \"max\"]]\n",
    "desc_path = os.path.join(DES_DIR, \"continuous_descriptive.xlsx\")\n",
    "desc.to_excel(desc_path, index=True)\n",
    "print(\"Saved:\", desc_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs\\des\\freq_school_type.xlsx\n",
      "Saved: outputs\\figs\\pie_school_type.png\n",
      "Saved: outputs\\des\\freq_gender.xlsx\n",
      "Saved: outputs\\figs\\pie_gender.png\n",
      "Saved: outputs\\des\\freq_grade_level.xlsx\n",
      "Saved: outputs\\figs\\pie_grade_level.png\n",
      "Saved: outputs\\des\\freq_major_cat.xlsx\n",
      "Saved: outputs\\figs\\pie_major_cat.png\n",
      "Saved: outputs\\des\\freq_hukou_type.xlsx\n",
      "Saved: outputs\\figs\\pie_hukou_type.png\n",
      "Saved: outputs\\des\\freq_teacher_ai_guidance.xlsx\n",
      "Saved: outputs\\figs\\pie_teacher_ai_guidance.png\n",
      "Saved: outputs\\des\\freq_peer_ai_norms.xlsx\n",
      "Saved: outputs\\figs\\pie_peer_ai_norms.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRANSLATION_MAP = {\n",
    "    \"school_type\": {\n",
    "        \"专科\": \"Vocational/Junior College\",\n",
    "        \"普通本科\": \"Regular Undergraduate\",\n",
    "        \"双一流\": \"Double First-Class\"\n",
    "    },\n",
    "    \"gender\": {\"男\": \"Male\", \"女\": \"Female\"},\n",
    "    \"grade_level\": {\"大一\": \"Freshman\", \"大二\": \"Sophomore\", \"大三\": \"Junior\", \"大四\": \"Senior\"},\n",
    "    \"major_cat\": {\n",
    "        \"计算机类\":\"Computer Science\",\n",
    "        \"电子信息类\":\"Electronics & Information\",\n",
    "        \"自动化类\":\"Automation\",\n",
    "        \"数学统计学类\":\"Mathematics & Statistics\",\n",
    "        \"物理学类\":\"Physics\",\n",
    "        \"化学类\":\"Chemistry\",\n",
    "        \"生物科学类\":\"Biological Sciences\",\n",
    "        \"土木类\":\"Civil Engineering\",\n",
    "        \"管理类\":\"Management\",\n",
    "        \"经济金融类\":\"Economics & Finance\",\n",
    "        \"文学类\":\"Literature\",\n",
    "        \"新闻传播学类\":\"Journalism & Communication\",\n",
    "        \"思想政治教育类\":\"Ideological & Political Education\",\n",
    "        \"体育学类\":\"Physical Education\",\n",
    "        \"医学类\":\"Medicine\",\n",
    "        \"教育学类\":\"Education\",\n",
    "        \"心理学类\":\"Psychology\",\n",
    "        \"法学类\":\"Law\",\n",
    "        \"历史学类\":\"History\",\n",
    "        \"艺术美术设计类\":\"Art & Design\"\n",
    "    },\n",
    "    \"hukou_type\": {\"城镇\":\"Urban\", \"农村\":\"Rural\"},\n",
    "    \"teacher_ai_guidance\": {\n",
    "        \"严格限制\":\"Strictly Restricted\",\n",
    "        \"允许但需注明\":\"Allowed with Attribution\",\n",
    "        \"鼓励规范使用\":\"Encouraged with Proper Use\"\n",
    "    },\n",
    "    \"peer_ai_norms\": {\"低\":\"Low\", \"中\":\"Medium\", \"高\":\"High\"}\n",
    "}\n",
    "\n",
    "# 需要翻译并画饼图的列（仅当这些原始字符串列存在时才处理）\n",
    "CATEGORICAL_STR_COLS_FOR_PIE = [\n",
    "    \"school_type\",\"gender\", \"grade_level\", \"major_cat\",\n",
    "    \"hukou_type\", \"teacher_ai_guidance\", \"peer_ai_norms\"\n",
    "]\n",
    "\n",
    "def translate_series_to_en(series: pd.Series, colname: str) -> pd.Series:\n",
    "    \"\"\"将分类变量的中文取值翻译成英文；未知值原样保留；缺失标记为 Missing。\"\"\"\n",
    "    mapping = TRANSLATION_MAP.get(colname, {})\n",
    "    return series.fillna(\"Missing\").map(lambda x: mapping.get(x, x))\n",
    "\n",
    "def save_freq_table_and_pie_translated(series: pd.Series, name: str):\n",
    "    # 先翻译\n",
    "    s_en = translate_series_to_en(series.astype(str), name)\n",
    "    # 频次与占比（英文标签）\n",
    "    vc = s_en.value_counts(dropna=False)\n",
    "    freq_df = pd.DataFrame({\"count\": vc, \"ratio\": vc / vc.sum()})\n",
    "    fpath = os.path.join(DES_DIR, f\"freq_{name}.xlsx\")\n",
    "    freq_df.to_excel(fpath)\n",
    "    print(\"Saved:\", fpath)\n",
    "\n",
    "    # 饼图（全英文+Times）\n",
    "    plt.figure(figsize=(6, 6), dpi=1200)\n",
    "    labels = [str(x) for x in vc.index]\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        x=vc.values,\n",
    "        labels=labels,\n",
    "        autopct=lambda p: f\"{p:.1f}%\",\n",
    "        startangle=90,\n",
    "        wedgeprops=dict(width=0.9, edgecolor='white')\n",
    "    )\n",
    "    plt.title(f\"Distribution of {name}\", fontsize=14)\n",
    "    plt.legend(wedges, labels, title=\"Categories\", loc=\"center left\", bbox_to_anchor=(1.0, 0.5))\n",
    "    fig_path = os.path.join(FIG_DIR, f\"pie_{name}.png\")\n",
    "    plt.savefig(fig_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", fig_path)\n",
    "\n",
    "# 执行：仅对存在的列进行输出\n",
    "for col in CATEGORICAL_STR_COLS_FOR_PIE:\n",
    "    if col in df_raw.columns:\n",
    "        save_freq_table_and_pie_translated(df_raw[col], col)\n",
    "    else:\n",
    "        print(f\"[Skip] '{col}' not found (string category).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs\\figs\\pie_vivid_gender.png\n",
      "Saved: outputs\\figs\\pie_vivid_grade_level.png\n",
      "Saved: outputs\\figs\\pie_vivid_major_cat.png\n",
      "Saved: outputs\\figs\\pie_vivid_school_type.png\n",
      "Saved: outputs\\figs\\pie_vivid_hukou_type.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ------------ 调色：高对比、绚丽且唯一 ------------\n",
    "def get_vivid_colors(n: int, for_major: bool = False):\n",
    "    \"\"\"\n",
    "    for_major=True：返回 20 个互不重复、相邻高对比的颜色（tab20 交错取色：偶数索引 + 奇数索引）\n",
    "    否则：n<=10 用 tab10；n>10 用 hsv 等分色环（强对比）\n",
    "    \"\"\"\n",
    "    if for_major:\n",
    "        # tab20 有 20 个色块（成对相近），为了提升相邻对比：先取偶数，再取奇数索引\n",
    "        base = list(cm.get_cmap(\"tab20\").colors)  # 20 tuples\n",
    "        order = list(range(0, 20, 2)) + list(range(1, 20, 2))\n",
    "        colors = [base[i] for i in order]\n",
    "        return colors[:n]\n",
    "    else:\n",
    "        if n <= 10:\n",
    "            return list(cm.get_cmap(\"tab10\").colors)[:n]\n",
    "        # 11+ 类别的备选（一般不会出现在本单元）：hsv 等分\n",
    "        hsv = cm.get_cmap(\"hsv\")\n",
    "        return [hsv(i / n) for i in range(n)]\n",
    "\n",
    "def plot_pie_no_labels(series_en: pd.Series, title: str, fig_path: str, legend_fontsize: int = 12):\n",
    "    \"\"\"绘制饼图：扇区仅百分比，无类别文字；类别在图例中；DPI=1200。\"\"\"\n",
    "    vc = series_en.value_counts(dropna=False)\n",
    "    labels = [str(x) for x in vc.index]\n",
    "    n = len(labels)\n",
    "\n",
    "    # 颜色\n",
    "    if title == \"major_cat\":\n",
    "        colors = get_vivid_colors(n, for_major=True)  # 20 个互不重复颜色\n",
    "    else:\n",
    "        colors = get_vivid_colors(n, for_major=False)\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure(figsize=(7, 6), dpi=1200)\n",
    "    wedges, _texts, autotexts = plt.pie(\n",
    "        x=vc.values,\n",
    "        labels=None,                      # 不在扇区边写类别\n",
    "        colors=colors,\n",
    "        autopct=lambda p: f\"{p:.1f}%\",    # 仅显示百分比\n",
    "        pctdistance=0.7,\n",
    "        startangle=90,\n",
    "        wedgeprops=dict(width=0.9, edgecolor='white', linewidth=0.8),\n",
    "        textprops={\"fontsize\": 6}   # ← 比例数字\n",
    "    )\n",
    "\n",
    "\n",
    "    # 图例放类别，字体更大\n",
    "    plt.legend(\n",
    "        wedges, labels,\n",
    "        title=\"Categories\",\n",
    "        loc=\"center left\", bbox_to_anchor=(1.02, 0.5),\n",
    "        fontsize=legend_fontsize, title_fontsize=legend_fontsize\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", fig_path)\n",
    "\n",
    "TARGET_PIE_COLS = [\"gender\", \"grade_level\", \"major_cat\", \"school_type\", \"hukou_type\"]\n",
    "# ------------ 执行：对指定列输出新的饼图 ------------\n",
    "for col in TARGET_PIE_COLS:\n",
    "    if col not in df_raw.columns:\n",
    "        print(f\"[Skip] '{col}' not found.\")\n",
    "        continue\n",
    "    s_en = translate_series_to_en(df_raw[col].astype(str), col)\n",
    "    out_path = os.path.join(FIG_DIR, f\"pie_vivid_{col}.png\")\n",
    "    plot_pie_no_labels(s_en, col, out_path, legend_fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs\\des\\missing_report_overall.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_rows</th>\n",
       "      <th>total_cols</th>\n",
       "      <th>total_cells</th>\n",
       "      <th>missing_cells</th>\n",
       "      <th>overall_missing_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>860</td>\n",
       "      <td>50</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_rows  total_cols  total_cells  missing_cells  overall_missing_ratio\n",
       "0         860          50        43000              0                    0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "total_cells = df_raw.shape[0] * df_raw.shape[1]\n",
    "missing_cells = int(df_raw.isna().sum().sum())\n",
    "overall_missing_ratio = missing_cells / total_cells if total_cells > 0 else 0.0\n",
    "\n",
    "miss_overall_df = pd.DataFrame({\n",
    "    \"total_rows\": [df_raw.shape[0]],\n",
    "    \"total_cols\": [df_raw.shape[1]],\n",
    "    \"total_cells\": [total_cells],\n",
    "    \"missing_cells\": [missing_cells],\n",
    "    \"overall_missing_ratio\": [overall_missing_ratio]\n",
    "})\n",
    "\n",
    "miss_path = os.path.join(DES_DIR, \"missing_report_overall.xlsx\")\n",
    "miss_overall_df.to_excel(miss_path, index=False)\n",
    "print(\"Saved:\", miss_path)\n",
    "miss_overall_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_type</th>\n",
       "      <th>school_type_encode</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender_encode</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>grade_level_encode</th>\n",
       "      <th>major_cat</th>\n",
       "      <th>major_cat_encode</th>\n",
       "      <th>hukou_type</th>\n",
       "      <th>hukou_type_encode</th>\n",
       "      <th>...</th>\n",
       "      <th>peer_ai_norms</th>\n",
       "      <th>course_difficulty</th>\n",
       "      <th>ai_paid_sub</th>\n",
       "      <th>dai_item_1</th>\n",
       "      <th>dai_item_2</th>\n",
       "      <th>dai_item_3</th>\n",
       "      <th>dai_item_4</th>\n",
       "      <th>dai_item_5</th>\n",
       "      <th>dai_item_6</th>\n",
       "      <th>ai_dep_dai_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>专科</td>\n",
       "      <td>0</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大四</td>\n",
       "      <td>4</td>\n",
       "      <td>文学类</td>\n",
       "      <td>10</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>男</td>\n",
       "      <td>0</td>\n",
       "      <td>大一</td>\n",
       "      <td>1</td>\n",
       "      <td>计算机类</td>\n",
       "      <td>0</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大一</td>\n",
       "      <td>1</td>\n",
       "      <td>化学类</td>\n",
       "      <td>5</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大一</td>\n",
       "      <td>1</td>\n",
       "      <td>化学类</td>\n",
       "      <td>5</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大二</td>\n",
       "      <td>2</td>\n",
       "      <td>思想政治教育类</td>\n",
       "      <td>12</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>专科</td>\n",
       "      <td>0</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大四</td>\n",
       "      <td>4</td>\n",
       "      <td>自动化类</td>\n",
       "      <td>2</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>双一流</td>\n",
       "      <td>2</td>\n",
       "      <td>男</td>\n",
       "      <td>0</td>\n",
       "      <td>大三</td>\n",
       "      <td>3</td>\n",
       "      <td>教育学类</td>\n",
       "      <td>15</td>\n",
       "      <td>农村</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大一</td>\n",
       "      <td>1</td>\n",
       "      <td>新闻传播学类</td>\n",
       "      <td>11</td>\n",
       "      <td>城镇</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>双一流</td>\n",
       "      <td>2</td>\n",
       "      <td>女</td>\n",
       "      <td>1</td>\n",
       "      <td>大二</td>\n",
       "      <td>2</td>\n",
       "      <td>计算机类</td>\n",
       "      <td>0</td>\n",
       "      <td>城镇</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>普通本科</td>\n",
       "      <td>1</td>\n",
       "      <td>男</td>\n",
       "      <td>0</td>\n",
       "      <td>大三</td>\n",
       "      <td>3</td>\n",
       "      <td>体育学类</td>\n",
       "      <td>13</td>\n",
       "      <td>城镇</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school_type  school_type_encode gender  gender_encode grade_level  \\\n",
       "0            专科                   0      女              1          大四   \n",
       "1          普通本科                   1      男              0          大一   \n",
       "2          普通本科                   1      女              1          大一   \n",
       "3          普通本科                   1      女              1          大一   \n",
       "4          普通本科                   1      女              1          大二   \n",
       "..          ...                 ...    ...            ...         ...   \n",
       "855          专科                   0      女              1          大四   \n",
       "856         双一流                   2      男              0          大三   \n",
       "857        普通本科                   1      女              1          大一   \n",
       "858         双一流                   2      女              1          大二   \n",
       "859        普通本科                   1      男              0          大三   \n",
       "\n",
       "     grade_level_encode major_cat  major_cat_encode hukou_type  \\\n",
       "0                     4       文学类                10         农村   \n",
       "1                     1      计算机类                 0         农村   \n",
       "2                     1       化学类                 5         农村   \n",
       "3                     1       化学类                 5         农村   \n",
       "4                     2   思想政治教育类                12         农村   \n",
       "..                  ...       ...               ...        ...   \n",
       "855                   4      自动化类                 2         农村   \n",
       "856                   3      教育学类                15         农村   \n",
       "857                   1    新闻传播学类                11         城镇   \n",
       "858                   2      计算机类                 0         城镇   \n",
       "859                   3      体育学类                13         城镇   \n",
       "\n",
       "     hukou_type_encode  ...  peer_ai_norms  course_difficulty  ai_paid_sub  \\\n",
       "0                    0  ...              1                  4            0   \n",
       "1                    0  ...              3                  2            0   \n",
       "2                    0  ...              1                  1            0   \n",
       "3                    0  ...              5                  3            1   \n",
       "4                    0  ...              3                  3            0   \n",
       "..                 ...  ...            ...                ...          ...   \n",
       "855                  0  ...              5                  4            0   \n",
       "856                  0  ...              1                  2            1   \n",
       "857                  1  ...              3                  3            0   \n",
       "858                  1  ...              5                  4            1   \n",
       "859                  1  ...              3                  4            0   \n",
       "\n",
       "     dai_item_1  dai_item_2  dai_item_3  dai_item_4  dai_item_5  dai_item_6  \\\n",
       "0             1           0           0           1           0           0   \n",
       "1             4           5           3           1           4           1   \n",
       "2             3           3           4           1           1           5   \n",
       "3             4           4           1           4           4           2   \n",
       "4             5           1           3           0           1           0   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "855           2           5           1           2           0           1   \n",
       "856           3           0           5           3           1           2   \n",
       "857           3           0           2           3           1           2   \n",
       "858           0           4           1           4           4           3   \n",
       "859           3           2           4           3           5           2   \n",
       "\n",
       "     ai_dep_dai_total  \n",
       "0                   2  \n",
       "1                  18  \n",
       "2                  17  \n",
       "3                  19  \n",
       "4                  10  \n",
       "..                ...  \n",
       "855                11  \n",
       "856                14  \n",
       "857                11  \n",
       "858                16  \n",
       "859                19  \n",
       "\n",
       "[860 rows x 50 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三部分：机器学习模型建模与评估模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use  = df_raw.drop(['school_type','gender','grade_level','major_cat',\"hukou_type\",\"teacher_ai_guidance\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_type_encode</th>\n",
       "      <th>gender_encode</th>\n",
       "      <th>grade_level_encode</th>\n",
       "      <th>major_cat_encode</th>\n",
       "      <th>hukou_type_encode</th>\n",
       "      <th>age</th>\n",
       "      <th>parent_edu_f</th>\n",
       "      <th>parent_edu_m</th>\n",
       "      <th>family_income_quintile</th>\n",
       "      <th>ai_use_days_per_week</th>\n",
       "      <th>...</th>\n",
       "      <th>peer_ai_norms</th>\n",
       "      <th>course_difficulty</th>\n",
       "      <th>ai_paid_sub</th>\n",
       "      <th>dai_item_1</th>\n",
       "      <th>dai_item_2</th>\n",
       "      <th>dai_item_3</th>\n",
       "      <th>dai_item_4</th>\n",
       "      <th>dai_item_5</th>\n",
       "      <th>dai_item_6</th>\n",
       "      <th>ai_dep_dai_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     school_type_encode  gender_encode  grade_level_encode  major_cat_encode  \\\n",
       "0                     0              1                   4                10   \n",
       "1                     1              0                   1                 0   \n",
       "2                     1              1                   1                 5   \n",
       "3                     1              1                   1                 5   \n",
       "4                     1              1                   2                12   \n",
       "..                  ...            ...                 ...               ...   \n",
       "855                   0              1                   4                 2   \n",
       "856                   2              0                   3                15   \n",
       "857                   1              1                   1                11   \n",
       "858                   2              1                   2                 0   \n",
       "859                   1              0                   3                13   \n",
       "\n",
       "     hukou_type_encode  age  parent_edu_f  parent_edu_m  \\\n",
       "0                    0   21            11            11   \n",
       "1                    0   21            13             9   \n",
       "2                    0   20            10            10   \n",
       "3                    0   20            12             9   \n",
       "4                    0   22            12            13   \n",
       "..                 ...  ...           ...           ...   \n",
       "855                  0   19             7            10   \n",
       "856                  0   19            11            11   \n",
       "857                  1   20             7            11   \n",
       "858                  1   20            16            13   \n",
       "859                  1   22            12            10   \n",
       "\n",
       "     family_income_quintile  ai_use_days_per_week  ...  peer_ai_norms  \\\n",
       "0                         4                     4  ...              1   \n",
       "1                         1                     3  ...              3   \n",
       "2                         3                     5  ...              1   \n",
       "3                         5                     6  ...              5   \n",
       "4                         3                     7  ...              3   \n",
       "..                      ...                   ...  ...            ...   \n",
       "855                       5                     7  ...              5   \n",
       "856                       3                     7  ...              1   \n",
       "857                       3                     4  ...              3   \n",
       "858                       1                     3  ...              5   \n",
       "859                       3                     4  ...              3   \n",
       "\n",
       "     course_difficulty  ai_paid_sub  dai_item_1  dai_item_2  dai_item_3  \\\n",
       "0                    4            0           1           0           0   \n",
       "1                    2            0           4           5           3   \n",
       "2                    1            0           3           3           4   \n",
       "3                    3            1           4           4           1   \n",
       "4                    3            0           5           1           3   \n",
       "..                 ...          ...         ...         ...         ...   \n",
       "855                  4            0           2           5           1   \n",
       "856                  2            1           3           0           5   \n",
       "857                  3            0           3           0           2   \n",
       "858                  4            1           0           4           1   \n",
       "859                  4            0           3           2           4   \n",
       "\n",
       "     dai_item_4  dai_item_5  dai_item_6  ai_dep_dai_total  \n",
       "0             1           0           0                 2  \n",
       "1             1           4           1                18  \n",
       "2             1           1           5                17  \n",
       "3             4           4           2                19  \n",
       "4             0           1           0                10  \n",
       "..          ...         ...         ...               ...  \n",
       "855           2           0           1                11  \n",
       "856           3           1           2                14  \n",
       "857           3           1           2                11  \n",
       "858           4           4           3                16  \n",
       "859           3           5           2                19  \n",
       "\n",
       "[860 rows x 44 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(688, 37) (172, 37) (688,) (172,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = df_use.drop(columns=[TARGET], axis=1).copy()\n",
    "X = X.drop(columns=X.columns[-6:], axis=1)\n",
    "y = df_use[TARGET].copy()\n",
    "\n",
    "# 80/20 划分\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=SEED\n",
    ")\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_type_encode</th>\n",
       "      <th>gender_encode</th>\n",
       "      <th>grade_level_encode</th>\n",
       "      <th>major_cat_encode</th>\n",
       "      <th>hukou_type_encode</th>\n",
       "      <th>age</th>\n",
       "      <th>parent_edu_f</th>\n",
       "      <th>parent_edu_m</th>\n",
       "      <th>family_income_quintile</th>\n",
       "      <th>ai_use_days_per_week</th>\n",
       "      <th>...</th>\n",
       "      <th>ease_of_use</th>\n",
       "      <th>trust_in_ai</th>\n",
       "      <th>ai_lit_knowledge</th>\n",
       "      <th>ai_lit_skill</th>\n",
       "      <th>ai_lit_ethics</th>\n",
       "      <th>ai_literacy_index</th>\n",
       "      <th>teacher_ai_guidance_encode</th>\n",
       "      <th>peer_ai_norms</th>\n",
       "      <th>course_difficulty</th>\n",
       "      <th>ai_paid_sub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>51.6</td>\n",
       "      <td>74.2</td>\n",
       "      <td>63.0</td>\n",
       "      <td>62.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>73.6</td>\n",
       "      <td>69.2</td>\n",
       "      <td>66.6</td>\n",
       "      <td>70.4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>82.0</td>\n",
       "      <td>90.6</td>\n",
       "      <td>84.1</td>\n",
       "      <td>85.9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>69.5</td>\n",
       "      <td>79.8</td>\n",
       "      <td>59.8</td>\n",
       "      <td>71.7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>62.1</td>\n",
       "      <td>67.2</td>\n",
       "      <td>53.4</td>\n",
       "      <td>62.4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>78.8</td>\n",
       "      <td>77.7</td>\n",
       "      <td>47.1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>62.4</td>\n",
       "      <td>47.7</td>\n",
       "      <td>61.2</td>\n",
       "      <td>56.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>81.0</td>\n",
       "      <td>85.8</td>\n",
       "      <td>67.9</td>\n",
       "      <td>80.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>68.5</td>\n",
       "      <td>60.9</td>\n",
       "      <td>63.6</td>\n",
       "      <td>64.5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>72.1</td>\n",
       "      <td>76.1</td>\n",
       "      <td>80.7</td>\n",
       "      <td>75.4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>688 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     school_type_encode  gender_encode  grade_level_encode  major_cat_encode  \\\n",
       "442                   1              0                   1                 9   \n",
       "193                   1              0                   3                19   \n",
       "397                   2              1                   2                 0   \n",
       "348                   1              0                   3                 2   \n",
       "509                   1              1                   2                18   \n",
       "..                  ...            ...                 ...               ...   \n",
       "99                    1              0                   1                10   \n",
       "777                   1              1                   1                16   \n",
       "221                   2              0                   4                17   \n",
       "638                   2              1                   2                12   \n",
       "672                   2              1                   2                16   \n",
       "\n",
       "     hukou_type_encode  age  parent_edu_f  parent_edu_m  \\\n",
       "442                  1   21            15            13   \n",
       "193                  1   21            16            15   \n",
       "397                  0   20             8            13   \n",
       "348                  0   23            11            13   \n",
       "509                  1   21            12            14   \n",
       "..                 ...  ...           ...           ...   \n",
       "99                   1   19            11            10   \n",
       "777                  1   20            11            15   \n",
       "221                  1   18            17            11   \n",
       "638                  1   19            13            13   \n",
       "672                  1   20            14            12   \n",
       "\n",
       "     family_income_quintile  ai_use_days_per_week  ...  ease_of_use  \\\n",
       "442                       3                     6  ...            4   \n",
       "193                       3                     3  ...            4   \n",
       "397                       2                     5  ...            4   \n",
       "348                       2                     4  ...            2   \n",
       "509                       4                     5  ...            4   \n",
       "..                      ...                   ...  ...          ...   \n",
       "99                        5                     0  ...            4   \n",
       "777                       2                     4  ...            4   \n",
       "221                       2                     6  ...            4   \n",
       "638                       3                     4  ...            3   \n",
       "672                       2                     5  ...            3   \n",
       "\n",
       "     trust_in_ai  ai_lit_knowledge  ai_lit_skill  ai_lit_ethics  \\\n",
       "442            4              51.6          74.2           63.0   \n",
       "193            4              73.6          69.2           66.6   \n",
       "397            2              82.0          90.6           84.1   \n",
       "348            4              69.5          79.8           59.8   \n",
       "509            4              62.1          67.2           53.4   \n",
       "..           ...               ...           ...            ...   \n",
       "99             3              78.8          77.7           47.1   \n",
       "777            4              62.4          47.7           61.2   \n",
       "221            3              81.0          85.8           67.9   \n",
       "638            4              68.5          60.9           63.6   \n",
       "672            3              72.1          76.1           80.7   \n",
       "\n",
       "     ai_literacy_index  teacher_ai_guidance_encode  peer_ai_norms  \\\n",
       "442               62.9                           1              3   \n",
       "193               70.4                           2              5   \n",
       "397               85.9                           0              5   \n",
       "348               71.7                           0              3   \n",
       "509               62.4                           1              3   \n",
       "..                 ...                         ...            ...   \n",
       "99                72.0                           0              5   \n",
       "777               56.3                           1              1   \n",
       "221               80.3                           0              1   \n",
       "638               64.5                           0              5   \n",
       "672               75.4                           0              3   \n",
       "\n",
       "     course_difficulty  ai_paid_sub  \n",
       "442                  4            1  \n",
       "193                  5            0  \n",
       "397                  3            0  \n",
       "348                  3            0  \n",
       "509                  3            0  \n",
       "..                 ...          ...  \n",
       "99                   4            0  \n",
       "777                  5            1  \n",
       "221                  3            0  \n",
       "638                  4            0  \n",
       "672                  5            0  \n",
       "\n",
       "[688 rows x 37 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Helper functions for metrics, AIC/BIC proxy, and robustness\n",
    "# 中文注释：统一评估；AIC/BIC为非概率模型的近似（基于RSS与“参数规模”近似k）\n",
    "\n",
    "def count_params_proxy(model, X_fit=None):\n",
    "    \"\"\"近似参数规模k：不同模型采用可用的代理量\"\"\"\n",
    "    try:\n",
    "        # 线性\n",
    "        if hasattr(model, \"coef_\"):\n",
    "            k = int(np.count_nonzero(model.coef_) + (1 if getattr(model, \"fit_intercept\", True) else 0))\n",
    "            return max(k, 1)\n",
    "        # SVR: 支持向量数量\n",
    "        if hasattr(model, \"support_\"):\n",
    "            return max(len(model.support_), 1)\n",
    "        # KNN: 取特征数作为自由度近似\n",
    "        if model.__class__.__name__.lower().startswith(\"kneighbors\"):\n",
    "            return max(getattr(model, \"n_features_in_\", X_fit.shape[1] if X_fit is not None else 1), 1)\n",
    "        # 树模型（RF/ET）：节点总数近似\n",
    "        if hasattr(model, \"estimators_\"):\n",
    "            k = 0\n",
    "            for est in model.estimators_:\n",
    "                if hasattr(est, \"tree_\"):\n",
    "                    k += est.tree_.node_count\n",
    "            return max(k, 1)\n",
    "        # XGB: 近似 n_estimators * max_depth\n",
    "        if model.__class__.__name__ == \"XGBRegressor\":\n",
    "            return max(int(model.n_estimators * model.max_depth), 1)\n",
    "        # MLP：权重矩阵参数量\n",
    "        if hasattr(model, \"coefs_\"):\n",
    "            k = sum(w.size for w in model.coefs_) + sum(b.size for b in model.intercepts_)\n",
    "            return max(int(k), 1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 回退：特征数\n",
    "    return max(getattr(model, \"n_features_in_\", X_fit.shape[1] if X_fit is not None else 1), 1)\n",
    "\n",
    "def regression_report(y_true, y_pred, model=None, X_fit=None, model_name=\"\"):\n",
    "    n = len(y_true)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    k = count_params_proxy(model, X_fit)\n",
    "    # 高斯误差AIC/BIC近似： n*ln(RSS/n) + 2k / +k ln(n)\n",
    "    aic = n * np.log(rss / n + 1e-12) + 2 * k\n",
    "    bic = n * np.log(rss / n + 1e-12) + k * np.log(n)\n",
    "    return {\n",
    "        \"model\": model_name, \"R2\": r2, \"RMSE\": rmse, \"MAE\": mae,\n",
    "        \"AIC\": aic, \"BIC\": bic, \"k_params_proxy\": k, \"n_test\": n\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OLS ===\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[OLS] test R2=0.6244  RMSE=4.0810  Saved model→ outputs\\models\\best_OLS.pkl\n",
      "\n",
      "=== SVR ===\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "[SVR] test R2=0.6377  RMSE=4.0084  Saved model→ outputs\\models\\best_SVR.pkl\n",
      "\n",
      "=== KNN ===\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[KNN] test R2=0.4099  RMSE=5.1157  Saved model→ outputs\\models\\best_KNN.pkl\n",
      "\n",
      "=== ExtraTrees ===\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "[ExtraTrees] test R2=0.5779  RMSE=4.3265  Saved model→ outputs\\models\\best_ExtraTrees.pkl\n",
      "\n",
      "=== RandomForest ===\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "[RandomForest] test R2=0.5464  RMSE=4.4849  Saved model→ outputs\\models\\best_RandomForest.pkl\n",
      "\n",
      "=== XGB ===\n",
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n",
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n",
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n",
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n",
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n",
      "[XGB] test R2=0.6186  RMSE=4.1124  Saved model→ outputs\\models\\best_XGB.pkl\n",
      "\n",
      "=== ANN ===\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[ANN] test R2=0.6369  RMSE=4.0127  Saved model→ outputs\\models\\best_ANN.pkl\n",
      "Saved summary: outputs\\logs\\model_eval_summary.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "      <th>k_params_proxy</th>\n",
       "      <th>n_test</th>\n",
       "      <th>cv_best_mean_r2</th>\n",
       "      <th>cv_best_std_r2</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVR</td>\n",
       "      <td>0.637672</td>\n",
       "      <td>4.008439</td>\n",
       "      <td>3.217699</td>\n",
       "      <td>551.610208</td>\n",
       "      <td>668.067504</td>\n",
       "      <td>37</td>\n",
       "      <td>172</td>\n",
       "      <td>0.549614</td>\n",
       "      <td>0.010386</td>\n",
       "      <td>{'svr__C': 10, 'svr__epsilon': 0.3, 'svr__gamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.636903</td>\n",
       "      <td>4.012689</td>\n",
       "      <td>3.151889</td>\n",
       "      <td>551.974764</td>\n",
       "      <td>668.432059</td>\n",
       "      <td>37</td>\n",
       "      <td>172</td>\n",
       "      <td>0.528048</td>\n",
       "      <td>0.012277</td>\n",
       "      <td>{'mlp__activation': 'tanh', 'mlp__alpha': 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLS</td>\n",
       "      <td>0.624444</td>\n",
       "      <td>4.080957</td>\n",
       "      <td>3.345741</td>\n",
       "      <td>557.778002</td>\n",
       "      <td>674.235298</td>\n",
       "      <td>37</td>\n",
       "      <td>172</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.008616</td>\n",
       "      <td>{'lr__fit_intercept': True}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.618638</td>\n",
       "      <td>4.112376</td>\n",
       "      <td>3.439971</td>\n",
       "      <td>2886.416384</td>\n",
       "      <td>6663.409756</td>\n",
       "      <td>1200</td>\n",
       "      <td>172</td>\n",
       "      <td>0.512780</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>{'colsample_bytree': 0.6, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>0.577882</td>\n",
       "      <td>4.326544</td>\n",
       "      <td>3.573891</td>\n",
       "      <td>320851.880563</td>\n",
       "      <td>824998.660892</td>\n",
       "      <td>160174</td>\n",
       "      <td>172</td>\n",
       "      <td>0.448068</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.546408</td>\n",
       "      <td>4.484946</td>\n",
       "      <td>3.692945</td>\n",
       "      <td>180772.249892</td>\n",
       "      <td>464449.632098</td>\n",
       "      <td>90128</td>\n",
       "      <td>172</td>\n",
       "      <td>0.425941</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>{'max_depth': 20, 'max_features': 0.5, 'min_sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.409850</td>\n",
       "      <td>5.115705</td>\n",
       "      <td>4.077325</td>\n",
       "      <td>635.516403</td>\n",
       "      <td>751.973698</td>\n",
       "      <td>37</td>\n",
       "      <td>172</td>\n",
       "      <td>0.314632</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>{'knn__n_neighbors': 15, 'knn__p': 2, 'knn__we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model        R2      RMSE       MAE            AIC            BIC  \\\n",
       "1           SVR  0.637672  4.008439  3.217699     551.610208     668.067504   \n",
       "6           ANN  0.636903  4.012689  3.151889     551.974764     668.432059   \n",
       "0           OLS  0.624444  4.080957  3.345741     557.778002     674.235298   \n",
       "5           XGB  0.618638  4.112376  3.439971    2886.416384    6663.409756   \n",
       "3    ExtraTrees  0.577882  4.326544  3.573891  320851.880563  824998.660892   \n",
       "4  RandomForest  0.546408  4.484946  3.692945  180772.249892  464449.632098   \n",
       "2           KNN  0.409850  5.115705  4.077325     635.516403     751.973698   \n",
       "\n",
       "   k_params_proxy  n_test  cv_best_mean_r2  cv_best_std_r2  \\\n",
       "1              37     172         0.549614        0.010386   \n",
       "6              37     172         0.528048        0.012277   \n",
       "0              37     172         0.550725        0.008616   \n",
       "5            1200     172         0.512780        0.011686   \n",
       "3          160174     172         0.448068        0.006635   \n",
       "4           90128     172         0.425941        0.005789   \n",
       "2              37     172         0.314632        0.008117   \n",
       "\n",
       "                                         best_params  \n",
       "1  {'svr__C': 10, 'svr__epsilon': 0.3, 'svr__gamm...  \n",
       "6  {'mlp__activation': 'tanh', 'mlp__alpha': 0.00...  \n",
       "0                        {'lr__fit_intercept': True}  \n",
       "5  {'colsample_bytree': 0.6, 'learning_rate': 0.0...  \n",
       "3  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...  \n",
       "4  {'max_depth': 20, 'max_features': 0.5, 'min_sa...  \n",
       "2  {'knn__n_neighbors': 15, 'knn__p': 2, 'knn__we...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9: Grids and training function\n",
    "# 中文注释：定义模型与网格；每个模型做GridSearchCV；重复多次更换random_state进行鲁棒性测试\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_models_and_grids():\n",
    "    models = {}\n",
    "\n",
    "    # OLS(LinearRegression)：仅标准化做在模型前\n",
    "    models[\"OLS\"] = (\n",
    "        Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "            (\"lr\", LinearRegression())\n",
    "        ]),\n",
    "        {\"lr__fit_intercept\": [True, False]}\n",
    "    )\n",
    "\n",
    "    models[\"SVR\"] = (\n",
    "        Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svr\", SVR())\n",
    "        ]),\n",
    "        {\n",
    "            \"svr__kernel\": [\"rbf\"],\n",
    "            \"svr__C\": [1, 5, 10, 20, 50],\n",
    "            \"svr__gamma\": [\"scale\", 0.1, 0.01, 0.001],\n",
    "            \"svr__epsilon\": [0.1, 0.2, 0.3]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    models[\"KNN\"] = (\n",
    "        Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"knn\", KNeighborsRegressor())\n",
    "        ]),\n",
    "        {\n",
    "            \"knn__n_neighbors\": [3, 5, 7, 9, 15],\n",
    "            \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "            \"knn__p\": [1, 2]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    models[\"ExtraTrees\"] = (\n",
    "        ExtraTreesRegressor(random_state=SEED, n_jobs=-1),\n",
    "        {\n",
    "            \"n_estimators\": [200, 400, 800],\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    models[\"RandomForest\"] = (\n",
    "        RandomForestRegressor(random_state=SEED, n_jobs=-1),\n",
    "        {\n",
    "            \"n_estimators\": [300, 500, 800, 1000],\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4],\n",
    "            \"max_features\": [\"auto\", \"sqrt\", 0.5]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    models[\"XGB\"] = (\n",
    "        XGBRegressor(\n",
    "            objective=\"reg:squarederror\", random_state=SEED, n_jobs=-1, tree_method=\"hist\"\n",
    "        ),\n",
    "        {\n",
    "            \"n_estimators\": [400, 800, 1200],\n",
    "            \"max_depth\": [3, 5, 7, 9],\n",
    "            \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "            \"subsample\": [0.7, 0.9, 1.0],\n",
    "            \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "            \"reg_lambda\": [1.0, 3.0, 5.0]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    models[\"ANN\"] = (\n",
    "        Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"mlp\", MLPRegressor(max_iter=600, random_state=SEED))\n",
    "        ]),\n",
    "        {\n",
    "            \"mlp__hidden_layer_sizes\": [(64,), (128,), (64, 32), (128, 64)],\n",
    "            \"mlp__alpha\": [1e-5, 1e-4, 1e-3],\n",
    "            \"mlp__learning_rate_init\": [1e-3, 5e-4, 1e-4],\n",
    "            \"mlp__activation\": [\"relu\", \"tanh\"]\n",
    "        }\n",
    "    )\n",
    "    return models\n",
    "\n",
    "def train_and_evaluate_all(X_train, y_train, X_test, y_test, n_repeats=5):\n",
    "    models = get_models_and_grids()\n",
    "    all_results = []\n",
    "    best_models = {}\n",
    "    search_logs = {}\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for name, (estimator, grid) in models.items():\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        # 多次重复以测试鲁棒性（变更 cv 随机种子）\n",
    "        repeat_scores = []\n",
    "        best_est_global = None\n",
    "        best_score_global = -np.inf\n",
    "        best_params_global = None\n",
    "\n",
    "        for rep in range(n_repeats):\n",
    "            cv_rep = KFold(n_splits=5, shuffle=True, random_state=SEED + rep)\n",
    "            gs = GridSearchCV(\n",
    "                estimator, grid, scoring=\"r2\", cv=cv_rep, n_jobs=-1, verbose=1\n",
    "            )\n",
    "            gs.fit(X_train, y_train)\n",
    "            # 记录\n",
    "            repeat_scores.append(gs.best_score_)\n",
    "            if gs.best_score_ > best_score_global:\n",
    "                best_score_global = gs.best_score_\n",
    "                best_est_global = gs.best_estimator_\n",
    "                best_params_global = gs.best_params_\n",
    "\n",
    "        # 测试集评估\n",
    "        y_pred = best_est_global.predict(X_test)\n",
    "        rep = regression_report(y_test, y_pred, model=best_est_global, X_fit=X_train, model_name=name)\n",
    "        rep.update({\n",
    "            \"cv_best_mean_r2\": float(np.mean(repeat_scores)),\n",
    "            \"cv_best_std_r2\": float(np.std(repeat_scores)),\n",
    "            \"best_params\": str(best_params_global)\n",
    "        })\n",
    "        all_results.append(rep)\n",
    "\n",
    "        # 保存最优模型\n",
    "        model_path = os.path.join(MODEL_DIR, f\"best_{name}.pkl\")\n",
    "        joblib.dump(best_est_global, model_path)\n",
    "        best_models[name] = model_path\n",
    "\n",
    "        # 保存网格搜索日志（最后一次gs）\n",
    "        log_path = os.path.join(LOG_DIR, f\"gridcv_{name}.xlsx\")\n",
    "        pd.DataFrame(gs.cv_results_).to_excel(log_path, index=False)\n",
    "        search_logs[name] = log_path\n",
    "\n",
    "        print(f\"[{name}] test R2={rep['R2']:.4f}  RMSE={rep['RMSE']:.4f}  Saved model→ {model_path}\")\n",
    "\n",
    "    results_df = pd.DataFrame(all_results).sort_values(\"R2\", ascending=False)\n",
    "    res_path = os.path.join(LOG_DIR, \"model_eval_summary.xlsx\")\n",
    "    results_df.to_excel(res_path, index=False)\n",
    "    print(\"Saved summary:\", res_path)\n",
    "    return results_df, best_models, search_logs\n",
    "\n",
    "results_df, best_models, search_logs = train_and_evaluate_all(X_train, y_train, X_test, y_test, n_repeats=5)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习性能比较可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs\\figs\\compare_R2.png\n",
      "Saved: outputs\\figs\\compare_RMSE.png\n",
      "Saved: outputs\\figs\\compare_MAE.png\n",
      "Saved: outputs\\figs\\compare_AIC.png\n",
      "Saved: outputs\\figs\\compare_BIC.png\n"
     ]
    }
   ],
   "source": [
    "# # Cell 10: Bar plots of metrics across models\n",
    "# # 中文注释：绘制每个模型的R2、RMSE、MAE、AIC、BIC条形对比图；英文、Times、DPI=1200\n",
    "# plt_metrics = [\"R2\", \"RMSE\", \"MAE\", \"AIC\", \"BIC\"]\n",
    "# models_order = results_df[\"model\"].tolist()\n",
    "\n",
    "# for m in plt_metrics:\n",
    "#     plt.figure(figsize=(8, 5), dpi=1200)\n",
    "#     vals = results_df.set_index(\"model\").loc[models_order, m]\n",
    "#     ax = vals.plot(kind=\"bar\", color=\"#1f77b4\", edgecolor=\"black\")\n",
    "#     plt.title(f\"Model Comparison: {m}\", fontsize=14)\n",
    "#     plt.ylabel(m, fontsize=12)\n",
    "#     plt.xlabel(\"Model\", fontsize=12)\n",
    "#     plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "#     plt.xticks(rotation=45, ha=\"right\")\n",
    "#     plt.tight_layout()\n",
    "#     fig_path = os.path.join(FIG_DIR, f\"compare_{m}.png\")\n",
    "#     plt.savefig(fig_path, bbox_inches=\"tight\")\n",
    "#     plt.close()\n",
    "#     print(\"Saved:\", fig_path)\n",
    "\n",
    "\n",
    "plt_metrics = [\"R2\", \"RMSE\", \"MAE\", \"AIC\", \"BIC\"]\n",
    "\n",
    "# 新增：不参与可视化的模型\n",
    "exclude_models = {\"SVR\", \"ANN\", \"OLS\"}\n",
    "\n",
    "# 过滤顺序，保留原始顺序但去掉指定模型\n",
    "models_order = [m for m in results_df[\"model\"].tolist() if m not in exclude_models]\n",
    "\n",
    "for m in plt_metrics:\n",
    "    # 若过滤后没有可画的模型，跳过该指标\n",
    "    if len(models_order) == 0:\n",
    "        print(f\"[Skip] No models left to plot for metric: {m}\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(8, 5), dpi=1200)\n",
    "    vals = results_df.set_index(\"model\").loc[models_order, m]\n",
    "    ax = vals.plot(kind=\"bar\", color=\"#1f77b4\", edgecolor=\"black\")\n",
    "    plt.title(f\"Model Comparison: {m}\", fontsize=14)\n",
    "    plt.ylabel(m, fontsize=12)\n",
    "    plt.xlabel(\"Model\", fontsize=12)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(FIG_DIR, f\"compare_{m}.png\")\n",
    "    plt.savefig(fig_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", fig_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs\\figs\\scatter_OLS.png\n",
      "Saved: outputs\\figs\\scatter_SVR.png\n",
      "Saved: outputs\\figs\\scatter_KNN.png\n",
      "Saved: outputs\\figs\\scatter_ExtraTrees.png\n",
      "Saved: outputs\\figs\\scatter_RandomForest.png\n",
      "Saved: outputs\\figs\\scatter_XGB.png\n",
      "Saved: outputs\\figs\\scatter_ANN.png\n"
     ]
    }
   ],
   "source": [
    "# # Cell 11: Prediction vs. Actual scatter for each best model (with CI and line)\n",
    "# # 中文注释：仿照你给的范例，但全英文标签并设置Times与高DPI\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# def plot_pred_vs_actual(model, model_name, X_test, y_test, fig_path):\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_test = np.asarray(y_test)\n",
    "\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     mae = mean_absolute_error(y_test, y_pred)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     r2  = r2_score(y_test, y_pred)\n",
    "\n",
    "#     plt.figure(figsize=(8, 7), dpi=1200)\n",
    "#     scatter = plt.scatter(y_test, y_pred, c=y_test, cmap='viridis', alpha=0.7, edgecolors='k')\n",
    "#     plt.colorbar(scatter, label='Observed')\n",
    "\n",
    "#     # 对角线\n",
    "#     mn, mx = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
    "#     plt.plot([mn, mx], [mn, mx], 'purple', linestyle='--', label='Predicted = Observed')\n",
    "\n",
    "#     # 线性拟合\n",
    "#     reg = LinearRegression().fit(y_test.reshape(-1, 1), y_pred)\n",
    "#     y_lin_pred = reg.predict(y_test.reshape(-1, 1))\n",
    "#     plt.plot(y_test, y_lin_pred, color='red', linestyle='-', label='Fitted line')\n",
    "\n",
    "#     # 95% CI\n",
    "#     se = np.sqrt(np.sum((y_pred - y_lin_pred) ** 2) / (len(y_test) - 2))\n",
    "#     t_val = 1.96\n",
    "#     ci = t_val * se * np.sqrt(1/len(y_test) + (y_test - np.mean(y_test))**2 / np.sum((y_test - np.mean(y_test))**2))\n",
    "#     order = np.argsort(y_test)\n",
    "#     plt.fill_between(y_test[order], y_lin_pred[order] - ci[order], y_lin_pred[order] + ci[order],\n",
    "#                      color='pink', alpha=0.4, label='95% Confidence Interval')\n",
    "\n",
    "#     # 文字框（英文）\n",
    "#     txt = f\"$R^2$={r2:.3f}\\nRMSE={rmse:.3f}\\nMAE={mae:.3f}\"\n",
    "#     plt.text(0.02, 0.98, txt, transform=plt.gca().transAxes, va='top', ha='left',\n",
    "#              fontsize=12, bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8))\n",
    "\n",
    "#     plt.title(f\"{model_name}: Predicted vs Observed\", fontsize=15)\n",
    "#     plt.xlabel(\"Observed\", fontsize=12)\n",
    "#     plt.ylabel(\"Predicted\", fontsize=12)\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(fig_path, bbox_inches=\"tight\")\n",
    "#     plt.close()\n",
    "\n",
    "# # 对所有最佳模型出图\n",
    "# for name, path in best_models.items():\n",
    "#     est = joblib.load(path)\n",
    "#     fig_path = os.path.join(FIG_DIR, f\"scatter_{name}.png\")\n",
    "#     plot_pred_vs_actual(est, name, X_test, y_test, fig_path)\n",
    "#     print(\"Saved:\", fig_path)\n",
    "\n",
    "\n",
    "\n",
    "# Cell 11: Prediction vs. Actual scatter for each best model (with CI and line)\n",
    "# 中文注释：在注释框中增加 AIC、BIC、k_params_proxy；英文、Times、DPI=1200\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "def plot_pred_vs_actual(model, model_name, X_test, y_test, fig_path, k_params_proxy=None):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    # 基础指标\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2  = r2_score(y_test, y_pred)\n",
    "\n",
    "    # ===== 新增：AIC / BIC / k_params_proxy =====\n",
    "    # 以高斯残差为假设：loglik = -n/2 * [log(2πσ^2) + 1]，σ^2=RSS/n\n",
    "    n   = len(y_test)\n",
    "    rss = np.sum((y_test - y_pred) ** 2)\n",
    "    sigma2 = rss / n if n > 0 else 1.0\n",
    "    loglik = -0.5 * n * (np.log(2 * np.pi * sigma2) + 1.0)\n",
    "\n",
    "    # k 参数的“代理”：若未传入，则用模型的 n_features_in_（或 X_test 列数）作为近似\n",
    "    if k_params_proxy is None:\n",
    "        k_params_proxy = int(getattr(model, \"n_features_in_\", X_test.shape[1]))\n",
    "\n",
    "    AIC = 2 * k_params_proxy - 2 * loglik\n",
    "    BIC = np.log(n) * k_params_proxy - 2 * loglik\n",
    "    # ============================================\n",
    "\n",
    "    plt.figure(figsize=(8, 7), dpi=1200)\n",
    "    scatter = plt.scatter(y_test, y_pred, c=y_test, cmap='viridis', alpha=0.7, edgecolors='k')\n",
    "    plt.colorbar(scatter, label='Observed')\n",
    "\n",
    "    # 对角线\n",
    "    mn, mx = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
    "    plt.plot([mn, mx], [mn, mx], 'purple', linestyle='--', label='Predicted = Observed')\n",
    "\n",
    "    # 线性拟合\n",
    "    reg = LinearRegression().fit(y_test.reshape(-1, 1), y_pred)\n",
    "    y_lin_pred = reg.predict(y_test.reshape(-1, 1))\n",
    "    plt.plot(y_test, y_lin_pred, color='red', linestyle='-', label='Fitted line')\n",
    "\n",
    "    # 95% CI\n",
    "    se = np.sqrt(np.sum((y_pred - y_lin_pred) ** 2) / (len(y_test) - 2))\n",
    "    t_val = 1.96\n",
    "    ci = t_val * se * np.sqrt(1/len(y_test) + (y_test - np.mean(y_test))**2 / np.sum((y_test - np.mean(y_test))**2))\n",
    "    order = np.argsort(y_test)\n",
    "    plt.fill_between(y_test[order], y_lin_pred[order] - ci[order], y_lin_pred[order] + ci[order],\n",
    "                     color='pink', alpha=0.4, label='95% Confidence Interval')\n",
    "\n",
    "    # 文字框（英文）—— 增加 AIC / BIC / k\n",
    "    txt = (\n",
    "        f\"$R^2$={r2:.3f}\\n\"\n",
    "        f\"RMSE={rmse:.3f}\\n\"\n",
    "        f\"MAE={mae:.3f}\\n\"\n",
    "        f\"AIC={AIC:.1f}\\n\"\n",
    "        f\"BIC={BIC:.1f}\\n\"\n",
    "        f\"k_params={k_params_proxy}\"\n",
    "    )\n",
    "    plt.text(0.02, 0.98, txt, transform=plt.gca().transAxes, va='top', ha='left',\n",
    "             fontsize=12, bbox=dict(facecolor='white', edgecolor='gray', alpha=0.85))\n",
    "\n",
    "    plt.title(f\"{model_name}: Predicted vs Observed\", fontsize=15)\n",
    "    plt.xlabel(\"Observed\", fontsize=12)\n",
    "    plt.ylabel(\"Predicted\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# 若你在 results_df 里已经算过 k 的代理（列名假设为 k_params_proxy），可以传入使用更精确的值：\n",
    "# 例如：\n",
    "# k_map = dict(zip(results_df[\"model\"], results_df[\"k_params_proxy\"]))\n",
    "\n",
    "# 对所有最佳模型出图（有 k_map 就传，没有就不传）\n",
    "# 例：k_map = dict(zip(results_df[\"model\"], results_df[\"k_params_proxy\"]))  # 如无该列可注释掉\n",
    "k_map = {m: int(k) for m, k in zip(results_df[\"model\"], results_df.get(\"k_params_proxy\", [None]*len(results_df)))}\n",
    "\n",
    "for name, path in best_models.items():\n",
    "    est = joblib.load(path)\n",
    "    fig_path = os.path.join(FIG_DIR, f\"scatter_{name}.png\")\n",
    "    plot_pred_vs_actual(est, name, X_test, y_test, fig_path, k_params_proxy=k_map.get(name))\n",
    "    print(\"Saved:\", fig_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于SHAP的可解释分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded XGB model: XGB @ outputs\\models\\best_XGB.pkl\n",
      "X_test shape: (172, 37) y_test shape: (172,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# 统一英文字体与高分辨率\n",
    "rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "rcParams[\"figure.dpi\"] = 1200\n",
    "\n",
    "\n",
    "FIG_DIR = \"./outputs/shap\"; DES_DIR = \"./outputs/shap\"; os.makedirs(FIG_DIR, exist_ok=True); os.makedirs(DES_DIR, exist_ok=True)\n",
    "\n",
    "# 从你之前保存的 best_models 中抓取 XGB 模型\n",
    "# 例如 best_models = {\"XGB\": \"models/best_XGB.pkl\", ...}\n",
    "xgb_key_candidates = [k for k in best_models.keys() if \"xgb\" in k.lower() or \"xgboost\" in k.lower() or \"xgbregressor\" in k.lower()]\n",
    "assert len(xgb_key_candidates) >= 1, \"best_models 里未找到 XGB 模型键名，请检查。\"\n",
    "XGB_NAME = xgb_key_candidates[0]\n",
    "XGB_PATH = best_models[XGB_NAME]\n",
    "xgb_est = joblib.load(XGB_PATH)\n",
    "\n",
    "# 保证特征名可用\n",
    "if hasattr(X_test, \"columns\"):\n",
    "    FEATURE_COLS = list(X_test.columns)\n",
    "else:\n",
    "    FEATURE_COLS = [f\"feat_{i}\" for i in range(X_test.shape[1])]\n",
    "\n",
    "# 强制 y_test 为 ndarray，便于后续索引\n",
    "y_test_arr = np.asarray(y_test)\n",
    "\n",
    "print(f\"[OK] Loaded XGB model: {XGB_NAME} @ {XGB_PATH}\")\n",
    "print(\"X_test shape:\", X_test.shape, \"y_test shape:\", y_test_arr.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./outputs/shap\\shap_global_importance_xgb.xlsx\n",
      "Saved: ./outputs/shap\\shap_summary_beeswarm_xgb.png\n",
      "Saved: ./outputs/shap\\shap_summary_bar_xgb.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TreeExplainer（XGB 原生支持）\n",
    "explainer = shap.TreeExplainer(xgb_est)\n",
    "# 新版 shap 推荐 explainer(X) 得到 Explanation；旧版返回 ndarray。两种都兼容：\n",
    "shap_expl = explainer(X_test)\n",
    "\n",
    "# 统一拿 values / base values / data\n",
    "if isinstance(shap_expl, shap.Explanation):\n",
    "    shap_values = shap_expl.values            # (n, p)\n",
    "    base_values = shap_expl.base_values       # (n,)\n",
    "    data_used   = shap_expl.data              # (n, p)\n",
    "else:\n",
    "    shap_values = shap_expl                   # ndarray\n",
    "    base_values = np.repeat(getattr(explainer, \"expected_value\", 0.0), X_test.shape[0])\n",
    "    data_used = X_test\n",
    "\n",
    "# === 导出全局特征重要性（|SHAP| 均值）到 Excel ===\n",
    "abs_mean_importance = np.mean(np.abs(shap_values), axis=0)\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": FEATURE_COLS,\n",
    "    \"mean_abs_shap\": abs_mean_importance\n",
    "}).sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "imp_path = os.path.join(DES_DIR, \"shap_global_importance_xgb.xlsx\")\n",
    "imp_df.to_excel(imp_path, index=False)\n",
    "print(\"Saved:\", imp_path)\n",
    "\n",
    "# === 全局图 1：summary beeswarm ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.summary_plot(shap_values, data_used, feature_names=FEATURE_COLS, show=False, plot_size=None, max_display=30)\n",
    "plt.title(\"SHAP Summary (Beeswarm) - XGB\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(FIG_DIR, \"shap_summary_beeswarm_xgb.png\")\n",
    "plt.savefig(fig_path, bbox_inches=\"tight\", dpi=1200)\n",
    "plt.close()\n",
    "print(\"Saved:\", fig_path)\n",
    "\n",
    "# === 全局图 2：summary bar（平均 |SHAP|）===\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.summary_plot(shap_values, data_used, feature_names=FEATURE_COLS, plot_type=\"bar\", show=False, plot_size=None, max_display=30)\n",
    "plt.title(\"SHAP Summary (Bar) - XGB\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(FIG_DIR, \"shap_summary_bar_xgb.png\")\n",
    "plt.savefig(fig_path, bbox_inches=\"tight\", dpi=1200)\n",
    "plt.close()\n",
    "print(\"Saved:\", fig_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./outputs/shap\\shap_dependence_interaction_peer_ai_norms.png\n",
      "Saved: ./outputs/shap\\shap_dependence_interaction_ai_daily_minutes.png\n",
      "Saved: ./outputs/shap\\shap_dependence_interaction_academic_stress.png\n",
      "Saved: ./outputs/shap\\shap_dependence_interaction_trust_in_ai.png\n",
      "Saved: ./outputs/shap\\shap_dependence_interaction_grade_level_encode.png\n",
      "Saved: ./outputs/shap\\shap_dependence_interaction_ucla_loneliness.png\n",
      "Saved: ./outputs/shap\\shap_dependence_interaction_acad_self_efficacy.png\n",
      "Saved: ./outputs/shap\\shap_dependence_interaction_school_type_encode.png\n",
      "Saved: ./outputs/shap\\shap_interaction_matrix_xgb.xlsx\n",
      "Saved: ./outputs/shap\\shap_interaction_heatmap_xgb.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 8400x6000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 8400x6000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 8400x6000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 8400x6000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 8400x6000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 8400x6000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 8400x6000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 8400x6000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 1) 选择前若干重要特征，画带交互的 dependence plot\n",
    "topK = 8  # 你可改为 10 或其他\n",
    "top_feats = imp_df[\"feature\"].head(topK).tolist()\n",
    "\n",
    "for feat in top_feats:\n",
    "    # interaction_index=\"auto\" 让 shap 自动选择与当前特征交互最强的另一个特征来着色\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    shap.dependence_plot(feat, shap_values, data_used, feature_names=FEATURE_COLS,\n",
    "                         interaction_index=\"auto\", show=False)\n",
    "    plt.title(f\"SHAP Dependence with Interaction — {feat}\", fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    outp = os.path.join(FIG_DIR, f\"shap_dependence_interaction_{feat}.png\")\n",
    "    plt.savefig(outp, bbox_inches=\"tight\", dpi=1200)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", outp)\n",
    "\n",
    "# 2) 交互值矩阵（仅树模型支持 shap_interaction_values）\n",
    "try:\n",
    "    inter_vals = explainer.shap_interaction_values(X_test)   # (n, p, p)\n",
    "    # 平均绝对交互强度\n",
    "    mean_abs_inter = np.mean(np.abs(inter_vals), axis=0)     # (p, p)\n",
    "    inter_df = pd.DataFrame(mean_abs_inter, index=FEATURE_COLS, columns=FEATURE_COLS)\n",
    "    inter_path = os.path.join(DES_DIR, \"shap_interaction_matrix_xgb.xlsx\")\n",
    "    inter_df.to_excel(inter_path)\n",
    "    print(\"Saved:\", inter_path)\n",
    "\n",
    "    # 热力图（用 matplotlib 绘制）\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 8), dpi=1200)\n",
    "    plt.imshow(mean_abs_inter, cmap=\"viridis\")\n",
    "    plt.title(\"Mean |SHAP Interaction| Heatmap - XGB\", fontsize=14)\n",
    "    plt.colorbar(label=\"Mean |Interaction|\")\n",
    "    plt.xticks(ticks=np.arange(len(FEATURE_COLS)), labels=FEATURE_COLS, rotation=90)\n",
    "    plt.yticks(ticks=np.arange(len(FEATURE_COLS)), labels=FEATURE_COLS)\n",
    "    plt.tight_layout()\n",
    "    heat_path = os.path.join(FIG_DIR, \"shap_interaction_heatmap_xgb.png\")\n",
    "    plt.savefig(heat_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", heat_path)\n",
    "except Exception as e:\n",
    "    print(\"[Warn] shap_interaction_values not available:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Y sample index(label): 522, value: 1.0\n",
      "Max-Y sample index(label): 457, value: 30.0\n",
      "Saved: ./outputs/shap\\shap_waterfall_minY_xgb.png\n",
      "Saved: ./outputs/shap\\shap_waterfall_maxY_xgb.png\n",
      "Saved: ./outputs/shap\\shap_force_minY_xgb.png\n",
      "Saved: ./outputs/shap\\shap_force_maxY_xgb.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 12000x1920 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 12000x1920 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 4: Local explanations for min-Y and max-Y samples (robust to pandas/numpy) ===\n",
    "\n",
    "# 判断是否都是 pandas 对象（有 index / iloc）\n",
    "use_pandas = hasattr(y_test, \"index\") and hasattr(X_test, \"iloc\")\n",
    "\n",
    "if use_pandas:\n",
    "    # pandas 分支：idxmin/idxmax 返回 “标签索引”\n",
    "    idx_min_label = y_test.idxmin()\n",
    "    idx_max_label = y_test.idxmax()\n",
    "    # 取对应一行（注意 loc 要用双中括号以保持二维）\n",
    "    x_min = X_test.loc[[idx_min_label]]\n",
    "    x_max = X_test.loc[[idx_max_label]]\n",
    "    # 真实 y 值用 label 取，避免把标签当位置\n",
    "    y_min_val = float(y_test.loc[idx_min_label])\n",
    "    y_max_val = float(y_test.loc[idx_max_label])\n",
    "    # 仅用于打印展示\n",
    "    print(f\"Min-Y sample index(label): {idx_min_label}, value: {y_min_val}\")\n",
    "    print(f\"Max-Y sample index(label): {idx_max_label}, value: {y_max_val}\")\n",
    "\n",
    "    # 计算这两点的 SHAP\n",
    "    exp_min = explainer(x_min)\n",
    "    exp_max = explainer(x_max)\n",
    "\n",
    "else:\n",
    "    # numpy 分支：用位置索引\n",
    "    idx_min_pos = int(np.argmin(y_test_arr))\n",
    "    idx_max_pos = int(np.argmax(y_test_arr))\n",
    "    x_min = X_test[idx_min_pos:idx_min_pos+1]\n",
    "    x_max = X_test[idx_max_pos:idx_max_pos+1]\n",
    "    y_min_val = float(y_test_arr[idx_min_pos])\n",
    "    y_max_val = float(y_test_arr[idx_max_pos])\n",
    "    print(f\"Min-Y sample position: {idx_min_pos}, value: {y_min_val}\")\n",
    "    print(f\"Max-Y sample position: {idx_max_pos}, value: {y_max_val}\")\n",
    "\n",
    "    exp_min = explainer(x_min)\n",
    "    exp_max = explainer(x_max)\n",
    "\n",
    "# —— Waterfall（局部解释）\n",
    "plt.figure(figsize=(8, 6), dpi=1200)\n",
    "shap.plots.waterfall(exp_min[0], max_display=20, show=False)\n",
    "plt.title(f\"Local Waterfall — Min-Y (y={y_min_val:.0f})\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "wf_min_path = os.path.join(FIG_DIR, \"shap_waterfall_minY_xgb.png\")\n",
    "plt.savefig(wf_min_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved:\", wf_min_path)\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=1200)\n",
    "shap.plots.waterfall(exp_max[0], max_display=20, show=False)\n",
    "plt.title(f\"Local Waterfall — Max-Y (y={y_max_val:.0f})\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "wf_max_path = os.path.join(FIG_DIR, \"shap_waterfall_maxY_xgb.png\")\n",
    "plt.savefig(wf_max_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved:\", wf_max_path)\n",
    "\n",
    "# —— Force plot（可选）\n",
    "try:\n",
    "    plt.figure(figsize=(10, 1.6), dpi=1200)\n",
    "    shap.force_plot(exp_min.base_values[0], exp_min.values[0], exp_min.data[0],\n",
    "                    feature_names=FEATURE_COLS, matplotlib=True, show=False)\n",
    "    plt.title(\"Local Force — Min-Y\", fontsize=12)\n",
    "    fp_min_path = os.path.join(FIG_DIR, \"shap_force_minY_xgb.png\")\n",
    "    plt.savefig(fp_min_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", fp_min_path)\n",
    "\n",
    "    plt.figure(figsize=(10, 1.6), dpi=1200)\n",
    "    shap.force_plot(exp_max.base_values[0], exp_max.values[0], exp_max.data[0],\n",
    "                    feature_names=FEATURE_COLS, matplotlib=True, show=False)\n",
    "    plt.title(\"Local Force — Max-Y\", fontsize=12)\n",
    "    fp_max_path = os.path.join(FIG_DIR, \"shap_force_maxY_xgb.png\")\n",
    "    plt.savefig(fp_max_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", fp_max_path)\n",
    "except Exception as e:\n",
    "    print(\"[Warn] Force plot PNG export failed (ok to ignore):\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于SEM模型的假设检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./outputs/sem/tables/measurement_diagnostics_kmo_bartlett_alpha.xlsx\n",
      "\n",
      "# Measurement（CFA）\n",
      "DAI =~ dai_item_1 + dai_item_2 + dai_item_3 + dai_item_4 + dai_item_5 + dai_item_6\n",
      "AI_Usage =~ ai_daily_minutes + ai_use_days_per_week + iter_per_task + prompt_length_avg\n",
      "Strain =~ academic_stress + ucla_loneliness\n",
      "AI_Literacy =~ ai_lit_knowledge + ai_lit_skill + ai_lit_ethics\n",
      "\n",
      "# Structural（H1：中介；保留直达）\n",
      "AI_Usage ~ peer_ai_norms + trust_in_ai\n",
      "DAI ~ AI_Usage + peer_ai_norms + trust_in_ai\n",
      "\n",
      "# H2：压力直接效应\n",
      "DAI ~ Strain\n",
      "\n",
      "# H3：素养缓冲（观测交互）\n",
      "DAI ~ usage_x_lit\n",
      "\n",
      "# Controls\n",
      "DAI ~ grade_level_encode + family_income_quintile\n",
      "\n",
      "Saved: ./outputs/sem/tables/sem_fit_main.xlsx\n",
      "Saved: ./outputs/sem/tables/sem_params_main.xlsx\n",
      "Saved: ./outputs/sem/tables/sem_paths_main_structural.xlsx\n",
      "Saved: ./outputs/sem/tables/sem_loadings.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./outputs/sem/tables/sem_boot_mediation_and_mod.xlsx\n",
      "Saved: ./outputs/sem/tables/sem_groups_school_type.xlsx\n",
      "Saved: ./outputs/sem/tables/sem_groups_major.xlsx\n",
      "Saved: ./outputs/sem/figs/sem_model_graph.png\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# SEM：诊断 → 建模 → 导出（按 H1→H2→H3）\n",
    "# =========================\n",
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# semopy 兼容导入\n",
    "from semopy import Model\n",
    "try:\n",
    "    from semopy.model_statistics import calc_stats\n",
    "except Exception:\n",
    "    try:\n",
    "        from semopy.statistics import calc_stats\n",
    "    except Exception:\n",
    "        from semopy import calc_stats\n",
    "try:\n",
    "    from semopy.inspector import inspect as sem_inspect\n",
    "except Exception:\n",
    "    from semopy.inspect import inspect as sem_inspect\n",
    "\n",
    "# 诊断：KMO/Bartlett\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo, calculate_bartlett_sphericity\n",
    "\n",
    "# 路径\n",
    "BASE = Path(\".\")\n",
    "DATA_PATH = Path(\"./data.xlsx\")\n",
    "OUT_DIRS = {\n",
    "    \"figs\": \"./outputs/sem/figs\",\n",
    "    \"tables\": \"./outputs/sem/tables\",\n",
    "    \"logs\": \"./outputs/sem/logs\",\n",
    "    \"models\": \"./outputs/sem/models\",\n",
    "}\n",
    "for p in OUT_DIRS.values():\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 20251010\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ========= 读取数据与派生列 =========\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "# 专业组别（用于异质性，不进主结构式）\n",
    "HIGH = {\"计算机类\",\"数学统计学类\",\"医学类\",\"管理类\",\"经济金融类\",\"新闻传播学类\",\"思想政治教育类\"}\n",
    "MID  = {\"物理学类\",\"电子信息类\",\"自动化类\",\"化学类\",\"生物科学类\",\"土木类\"}\n",
    "LOW  = {\"文学类\",\"教育学类\",\"心理学类\",\"法学类\",\"历史学类\",\"艺术美术设计类\",\"体育学类\"}\n",
    "\n",
    "def map_major_group(s):\n",
    "    if s in HIGH: return \"high\"\n",
    "    if s in MID:  return \"mid\"\n",
    "    return \"low\"\n",
    "\n",
    "if \"major_cat\" in df.columns:\n",
    "    df[\"major_group\"] = df[\"major_cat\"].apply(map_major_group)\n",
    "else:\n",
    "    df[\"major_group\"] = \"mid\"  # 若仅有编码，可后续再映射\n",
    "\n",
    "# —— 主模型所需列（与你的数据列一致）\n",
    "y_items    = [f\"dai_item_{i}\" for i in range(1, 7)]   # DAI 6题\n",
    "usage_inds = [\"ai_daily_minutes\", \"ai_use_days_per_week\", \"iter_per_task\", \"prompt_length_avg\"]\n",
    "strain_inds= [\"academic_stress\", \"ucla_loneliness\"]\n",
    "lit_inds   = [\"ai_lit_knowledge\", \"ai_lit_skill\", \"ai_lit_ethics\"]\n",
    "\n",
    "# 同伴列：优先使用 encode 版，否则回退\n",
    "peer_col = \"peer_ai_norms_encode\" if \"peer_ai_norms_encode\" in df.columns else \\\n",
    "           (\"peer_ai_norms\" if \"peer_ai_norms\" in df.columns else None)\n",
    "assert peer_col is not None, \"未找到同伴规范列（peer_ai_norms_encode 或 peer_ai_norms）\"\n",
    "\n",
    "exogenous = {\n",
    "    \"Peer\":   peer_col,\n",
    "    \"Trust\":  \"trust_in_ai\",\n",
    "    \"Grade\":  \"grade_level_encode\",\n",
    "    \"Income\": \"family_income_quintile\",\n",
    "}\n",
    "\n",
    "# 丢弃关键缺失\n",
    "cols_needed = y_items + usage_inds + strain_inds + lit_inds + list(exogenous.values())\n",
    "dff = df.dropna(subset=cols_needed).copy()\n",
    "\n",
    "# 居中处理（便于交互与稳定性）\n",
    "def center(s): return s - s.mean()\n",
    "for c in usage_inds + lit_inds:\n",
    "    dff[f\"c_{c}\"] = center(dff[c])\n",
    "\n",
    "# ========= 工具函数 =========\n",
    "def cronbach_alpha(df_):\n",
    "    df_ = df_.dropna()\n",
    "    k = df_.shape[1]\n",
    "    if k <= 1: return np.nan\n",
    "    var_sum = df_.var(axis=0, ddof=1).sum()\n",
    "    total_var = df_.sum(axis=1).var(ddof=1)\n",
    "    return (k/(k-1)) * (1 - var_sum/total_var) if total_var > 0 else np.nan\n",
    "\n",
    "def composite_z(df_, cols):\n",
    "    z = (df_[cols] - df_[cols].mean())/df_[cols].std(ddof=0)\n",
    "    return z.mean(axis=1)\n",
    "\n",
    "def _normalize_fit_stats(stats_obj, n_obs: int) -> pd.DataFrame:\n",
    "    if isinstance(stats_obj, pd.DataFrame):\n",
    "        df0 = stats_obj.copy()\n",
    "        if df0.shape[0] == 1:\n",
    "            row = df0.iloc[0]\n",
    "            def g(keys):\n",
    "                for k in keys:\n",
    "                    if k in row:\n",
    "                        try: return float(row[k])\n",
    "                        except Exception: pass\n",
    "                return None\n",
    "            out = {\"n_obs\": n_obs,\n",
    "                   \"df\": g([\"DoF\",\"dof\",\"df\"]), \"logl\": g([\"LogLik\",\"logl\",\"loglik\",\"LogLik.\"]),\n",
    "                   \"AIC\": g([\"AIC\",\"aic\"]), \"BIC\": g([\"BIC\",\"bic\"]),\n",
    "                   \"CFI\": g([\"CFI\",\"cfi\"]), \"TLI\": g([\"TLI\",\"tli\",\"NNFI\"]),\n",
    "                   \"RMSEA\": g([\"RMSEA\",\"rmsea\"]), \"SRMR\": g([\"SRMR\",\"srmr\"])}\n",
    "            return pd.DataFrame([out])\n",
    "        idx_lower = {str(i).lower(): i for i in df0.index}\n",
    "        def g2(key):\n",
    "            for cand in [key.lower(), key.upper(), key.capitalize()]:\n",
    "                if cand in idx_lower:\n",
    "                    s = df0.loc[idx_lower[cand]]\n",
    "                    try: return float(getattr(s, \"values\", [s])[0])\n",
    "                    except Exception:\n",
    "                        try: return float(s)\n",
    "                        except Exception: return None\n",
    "            return None\n",
    "        out = {\"n_obs\": n_obs, \"df\": g2(\"dof\"), \"logl\": g2(\"loglik\"),\n",
    "               \"AIC\": g2(\"aic\"), \"BIC\": g2(\"bic\"), \"CFI\": g2(\"cfi\"),\n",
    "               \"TLI\": g2(\"tli\"), \"RMSEA\": g2(\"rmsea\"), \"SRMR\": g2(\"srmr\")}\n",
    "        return pd.DataFrame([out])\n",
    "    out = {\"n_obs\": n_obs,\n",
    "           \"df\": getattr(stats_obj,\"dof\",None), \"logl\": getattr(stats_obj,\"logl\",None),\n",
    "           \"AIC\": getattr(stats_obj,\"aic\",None), \"BIC\": getattr(stats_obj,\"bic\",None),\n",
    "           \"CFI\": getattr(stats_obj,\"cfi\",None), \"TLI\": getattr(stats_obj,\"tli\",None),\n",
    "           \"RMSEA\": getattr(stats_obj,\"rmsea\",None), \"SRMR\": getattr(stats_obj,\"srmr\",None)}\n",
    "    return pd.DataFrame([out])\n",
    "\n",
    "def get_coef(params_df: pd.DataFrame, lhs: str, op: str, rhs: str) -> float:\n",
    "    if params_df is None or len(params_df) == 0: return np.nan\n",
    "    df0 = params_df.copy(); df0.columns = [str(c).lower() for c in df0.columns]\n",
    "    def pick(cands): \n",
    "        for c in cands:\n",
    "            if c in df0.columns: return c\n",
    "        return None\n",
    "    lcol = pick([\"lhs\",\"lval\",\"lvar\",\"left\"])\n",
    "    rcol = pick([\"rhs\",\"rval\",\"rvar\",\"right\"])\n",
    "    ocol = pick([\"op\",\"operator\"])\n",
    "    ecol = pick([\"std.estimate\",\"estimate\",\"est\",\"value\",\"beta\",\"coef\"])  # 优先取标准化\n",
    "    if not all([lcol,rcol,ocol,ecol]): return np.nan\n",
    "    sub = df0[(df0[lcol]==lhs) & (df0[ocol]==op) & (df0[rcol]==rhs)]\n",
    "    if sub.empty: return np.nan\n",
    "    try: return float(sub.iloc[0][ecol])\n",
    "    except Exception: return np.nan\n",
    "\n",
    "# ========= 1) 测量诊断：KMO / Bartlett / α =========\n",
    "blocks = {\n",
    "    \"DAI\": y_items,\n",
    "    \"AI_Usage\": usage_inds,\n",
    "    \"Strain\": strain_inds,\n",
    "    \"AI_Literacy\": lit_inds\n",
    "}\n",
    "diag_rows = []\n",
    "for name, cols in blocks.items():\n",
    "    sub = dff[cols].dropna()\n",
    "    kmo_per_var, kmo_total = calculate_kmo(sub)     # <- 改这里\n",
    "    chi2, p_bartlett = calculate_bartlett_sphericity(sub)\n",
    "    alpha = cronbach_alpha(sub)\n",
    "\n",
    "    diag_rows.append({\n",
    "        \"block\": name,\n",
    "        \"items\": \",\".join(cols),\n",
    "        \"n_used\": len(sub),\n",
    "        \"KMO_overall\": float(kmo_total),            # <- 用总体 KMO\n",
    "        \"KMO_mean_per_var\": float(np.nanmean(kmo_per_var)),  # 可选\n",
    "        \"Bartlett_chi2\": float(chi2),\n",
    "        \"Bartlett_p\": float(p_bartlett),\n",
    "        \"Cronbach_alpha\": float(alpha)\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "diag_df = pd.DataFrame(diag_rows)\n",
    "diag_path = \"./outputs/sem/tables/measurement_diagnostics_kmo_bartlett_alpha.xlsx\"\n",
    "diag_df.to_excel(diag_path, index=False)\n",
    "print(\"Saved:\", diag_path)\n",
    "\n",
    "# ========= 2) 构造 H3 的观测交互：usage_x_lit =========\n",
    "dff[\"_usage_idx_z\"] = composite_z(dff, usage_inds)\n",
    "dff[\"_lit_idx_z\"]   = composite_z(dff, lit_inds)\n",
    "dff[\"usage_x_lit\"]  = dff[\"_usage_idx_z\"] * dff[\"_lit_idx_z\"]\n",
    "\n",
    "# ========= 3) SEM 模型（无 UxL；按 H1→H2→H3）=========\n",
    "model_desc = f\"\"\"\n",
    "# Measurement（CFA）\n",
    "DAI =~ {' + '.join(y_items)}\n",
    "AI_Usage =~ {' + '.join(usage_inds)}\n",
    "Strain =~ {' + '.join(strain_inds)}\n",
    "AI_Literacy =~ {' + '.join(lit_inds)}\n",
    "\n",
    "# Structural（H1：中介；保留直达）\n",
    "AI_Usage ~ {exogenous['Peer']} + {exogenous['Trust']}\n",
    "DAI ~ AI_Usage + {exogenous['Peer']} + {exogenous['Trust']}\n",
    "\n",
    "# H2：压力直接效应\n",
    "DAI ~ Strain\n",
    "\n",
    "# H3：素养缓冲（观测交互）\n",
    "DAI ~ usage_x_lit\n",
    "\n",
    "# Controls\n",
    "DAI ~ {exogenous['Grade']} + {exogenous['Income']}\n",
    "\"\"\"\n",
    "print(model_desc)\n",
    "\n",
    "# ========= 4) 拟合主模型 & 导出 =========\n",
    "mod = Model(model_desc); _ = mod.fit(dff)\n",
    "stats_obj = calc_stats(mod)\n",
    "fit_df = _normalize_fit_stats(stats_obj, n_obs=len(dff))\n",
    "fit_path = \"./outputs/sem/tables/sem_fit_main.xlsx\"\n",
    "fit_df.to_excel(fit_path, index=False); print(\"Saved:\", fit_path)\n",
    "\n",
    "params = sem_inspect(mod)\n",
    "params_path = \"./outputs/sem/tables/sem_params_main.xlsx\"\n",
    "params.to_excel(params_path, index=False); print(\"Saved:\", params_path)\n",
    "\n",
    "# 结构路径与载荷导出（带显著性）\n",
    "def _pick_col(df, candidates, required=True):\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "    if required: raise KeyError(f\"None of columns found: {candidates}\")\n",
    "    return None\n",
    "\n",
    "dfp = params.copy()\n",
    "lhs_col = _pick_col(dfp, [\"lval\",\"lhs\"])\n",
    "rhs_col = _pick_col(dfp, [\"rval\",\"rhs\"])\n",
    "op_col  = _pick_col(dfp, [\"op\"])\n",
    "est_col = _pick_col(dfp, [\"Std.Estimate\",\"Estimate\",\"Est\",\"est\"])\n",
    "se_col  = _pick_col(dfp, [\"SE\",\"Std.Err\",\"StdErr\",\"SE (Robust)\",\"SE (robust)\"], required=False)\n",
    "z_col   = _pick_col(dfp, [\"z-value\",\"z\",\"Z\"], required=False)\n",
    "p_col   = _pick_col(dfp, [\"p-value\",\"P-value\",\"pvalue\",\"P(>|z|)\",\"p\"], required=False)\n",
    "\n",
    "ps = dfp[dfp[op_col]==\"~\"].copy()\n",
    "ps[\"_est\"] = pd.to_numeric(ps[est_col], errors=\"coerce\")\n",
    "ps[\"_se\"]  = pd.to_numeric(ps[se_col], errors=\"coerce\") if se_col in ps.columns else np.nan\n",
    "ps[\"_z\"]   = pd.to_numeric(ps[z_col], errors=\"coerce\") if z_col in ps.columns else np.nan\n",
    "ps[\"_p\"]   = pd.to_numeric(ps[p_col], errors=\"coerce\") if p_col in ps.columns else np.nan\n",
    "\n",
    "def _stars(p): \n",
    "    if pd.isna(p): return \"\"\n",
    "    return \"***\" if p<0.001 else (\"**\" if p<0.01 else (\"*\" if p<0.05 else \"\"))\n",
    "\n",
    "param_struct = ps[[lhs_col,op_col,rhs_col,\"_est\",\"_se\",\"_z\",\"_p\"]].rename(columns={\n",
    "    lhs_col:\"lhs\", op_col:\"op\", rhs_col:\"rhs\", \"_est\":\"beta\", \"_se\":\"se\", \"_z\":\"z\", \"_p\":\"p\"\n",
    "})\n",
    "param_struct[\"sig\"] = param_struct[\"p\"].apply(_stars)\n",
    "struct_path = \"./outputs/sem/tables/sem_paths_main_structural.xlsx\"\n",
    "param_struct.to_excel(struct_path, index=False); print(\"Saved:\", struct_path)\n",
    "\n",
    "pl = dfp[dfp[op_col] ==\"=~\"].copy()\n",
    "pl[\"_est\"] = pd.to_numeric(pl[est_col], errors=\"coerce\")\n",
    "loadings = pl[[lhs_col,op_col,rhs_col,\"_est\"]].rename(columns={\n",
    "    lhs_col:\"latent\", op_col:\"op\", rhs_col:\"indicator\", \"_est\":\"loading\"\n",
    "})\n",
    "load_path = \"./outputs/sem/tables/sem_loadings.xlsx\"\n",
    "loadings.to_excel(load_path, index=False); print(\"Saved:\", load_path)\n",
    "\n",
    "# ========= 5) 中介与调节 Bootstrap CI =========\n",
    "from tqdm.auto import trange\n",
    "idx_all = np.arange(len(dff))\n",
    "def _coef_from(m, lhs, op, rhs):\n",
    "    p = sem_inspect(m); return get_coef(p, lhs, op, rhs)\n",
    "\n",
    "b_peer_usage  = _coef_from(mod, \"AI_Usage\", \"~\", exogenous[\"Peer\"])\n",
    "b_trust_usage = _coef_from(mod, \"AI_Usage\", \"~\", exogenous[\"Trust\"])\n",
    "b_usage_dai   = _coef_from(mod, \"DAI\", \"~\", \"AI_Usage\")\n",
    "b_strain_dai  = _coef_from(mod, \"DAI\", \"~\", \"Strain\")\n",
    "b_int_dai     = _coef_from(mod, \"DAI\", \"~\", \"usage_x_lit\")\n",
    "\n",
    "ind_peer  = b_peer_usage * b_usage_dai\n",
    "ind_trust = b_trust_usage * b_usage_dai\n",
    "\n",
    "B = 1000; boot_records = []\n",
    "for _ in trange(B, leave=False):\n",
    "    boot_idx = np.random.choice(idx_all, size=len(idx_all), replace=True)\n",
    "    d_boot = dff.iloc[boot_idx].reset_index(drop=True)\n",
    "    try:\n",
    "        m_boot = Model(model_desc); m_boot.fit(d_boot)\n",
    "        bu  = _coef_from(m_boot, \"DAI\", \"~\", \"AI_Usage\")\n",
    "        bpu = _coef_from(m_boot, \"AI_Usage\", \"~\", exogenous[\"Peer\"])\n",
    "        btu = _coef_from(m_boot, \"AI_Usage\", \"~\", exogenous[\"Trust\"])\n",
    "        bsd = _coef_from(m_boot, \"DAI\", \"~\", \"Strain\")\n",
    "        bid = _coef_from(m_boot, \"DAI\", \"~\", \"usage_x_lit\")\n",
    "        boot_records.append({\n",
    "            \"ind_peer\": bpu*bu, \"ind_trust\": btu*bu,\n",
    "            \"b_strain_to_dai\": bsd, \"b_interaction_to_dai\": bid\n",
    "        })\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "boot_df = pd.DataFrame(boot_records)\n",
    "def ci(s): \n",
    "    return pd.Series({\"mean\": s.mean(), \"2.5%\": s.quantile(0.025), \"97.5%\": s.quantile(0.975)})\n",
    "ci_df = boot_df.apply(ci).T\n",
    "ci_df_path = \"./outputs/sem/tables/sem_boot_mediation_and_mod.xlsx\"\n",
    "ci_df.to_excel(ci_df_path); print(\"Saved:\", ci_df_path)\n",
    "\n",
    "# ========= 6) 分组比较：学校层级 & 专业强度 =========\n",
    "key_paths = [\n",
    "    (\"AI_Usage\",\"~\", exogenous[\"Peer\"]),\n",
    "    (\"AI_Usage\",\"~\", exogenous[\"Trust\"]),\n",
    "    (\"DAI\",\"~\",\"AI_Usage\"),\n",
    "    (\"DAI\",\"~\",\"Strain\"),\n",
    "    (\"DAI\",\"~\",\"usage_x_lit\"),\n",
    "    (\"DAI\",\"~\",exogenous[\"Grade\"]),\n",
    "    (\"DAI\",\"~\",exogenous[\"Income\"]),\n",
    "]\n",
    "\n",
    "def export_group_fit(group_col, fname):\n",
    "    rows = []\n",
    "    for g, dg in dff.groupby(group_col):\n",
    "        if len(dg) < 20:\n",
    "            print(f\"[Warn] group='{g}' 样本较小（n={len(dg)}）\"); \n",
    "        mg = Model(model_desc); mg.fit(dg)\n",
    "        pg = sem_inspect(mg); stats_g = calc_stats(mg)\n",
    "        fit_row = _normalize_fit_stats(stats_g, n_obs=len(dg)).iloc[0].to_dict()\n",
    "        for (lhs,op,rhs) in key_paths:\n",
    "            rows.append({\n",
    "                \"group\": f\"{group_col}={g}\",\n",
    "                \"lhs\": lhs, \"op\": op, \"rhs\": rhs,\n",
    "                \"coef\": get_coef(pg, lhs, op, rhs),\n",
    "                \"CFI\": fit_row.get(\"CFI\"), \"TLI\": fit_row.get(\"TLI\"),\n",
    "                \"RMSEA\": fit_row.get(\"RMSEA\"), \"SRMR\": fit_row.get(\"SRMR\"),\n",
    "                \"AIC\": fit_row.get(\"AIC\"), \"BIC\": fit_row.get(\"BIC\"),\n",
    "                \"n\": int(fit_row.get(\"n_obs\", len(dg)))\n",
    "            })\n",
    "    out = pd.DataFrame(rows); out.to_excel(fname, index=False); print(\"Saved:\", fname)\n",
    "\n",
    "assert \"school_type_encode\" in dff.columns, \"缺少 school_type_encode 列\"\n",
    "export_group_fit(\"school_type_encode\", \"./outputs/sem/tables/sem_groups_school_type.xlsx\")\n",
    "\n",
    "if \"major_group\" in dff.columns:\n",
    "    export_group_fit(\"major_group\", \"./outputs/sem/tables/sem_groups_major.xlsx\")\n",
    "\n",
    "# 结构图（需要本机安装 graphviz）\n",
    "from semopy import semplot\n",
    "fig_file = \"./outputs/sem/figs/sem_model_graph.png\"\n",
    "semplot(mod, fig_file); print(\"Saved:\", fig_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文注释：导入依赖并创建输出目录\n",
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from semopy import Model, Optimizer, calc_stats, semplot\n",
    "from semopy.inspector import inspect as sem_inspect\n",
    "\n",
    "# 路径\n",
    "BASE = Path(\".\")\n",
    "DATA_PATH = Path(\"./data.xlsx\")  # 你的上传路径\n",
    "OUT_DIRS = {\n",
    "    \"figs\": \"./outputs/sem/figs\",\n",
    "    \"tables\":  \"./outputs/sem/tables\",\n",
    "    \"logs\":  \"./outputs/sem/logs\",\n",
    "    \"models\":  \"./outputs/sem/models\",\n",
    "}\n",
    "\n",
    "\n",
    "RANDOM_STATE = 20251010\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready. N = 860\n"
     ]
    }
   ],
   "source": [
    "# 中文注释：读取数据；本数据是你手动清洗过的 data.xlsx（分类变量已编码）\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "# —— 构造“专业组别”major_group（高/中/低强度）用于异质性（不进入主模型）\n",
    "HIGH = {\"计算机类\",\"数学统计学类\",\"医学类\",\"管理类\",\"经济金融类\",\"新闻传播学类\",\"思想政治教育类\"}\n",
    "MID  = {\"物理学类\",\"电子信息类\",\"自动化类\",\"化学类\",\"生物科学类\",\"土木类\"}\n",
    "LOW  = {\"文学类\",\"教育学类\",\"心理学类\",\"法学类\",\"历史学类\",\"艺术美术设计类\",\"体育学类\"}\n",
    "\n",
    "def map_major_group(s):\n",
    "    if s in HIGH: return \"high\"\n",
    "    if s in MID:  return \"mid\"\n",
    "    return \"low\"\n",
    "\n",
    "# 若你的 df 中只保留了编码列，也可以用旧列名恢复；这里尽量兼容两种情况：\n",
    "if \"major_cat\" in df.columns:\n",
    "    df[\"major_group\"] = df[\"major_cat\"].apply(map_major_group)\n",
    "else:\n",
    "    # 若只有编码，先设置一个空的（可按编码再映射，你若需要我可以提供编码→中文→组别表）\n",
    "    df[\"major_group\"] = \"mid\"\n",
    "\n",
    "# —— 主模型需要的观测变量列名（与你生成的列保持一致）\n",
    "y_items = [f\"dai_item_{i}\" for i in range(1, 7)]\n",
    "usage_inds = [\"ai_daily_minutes\", \"ai_use_days_per_week\", \"iter_per_task\", \"prompt_length_avg\"]\n",
    "strain_inds = [\"academic_stress\", \"ucla_loneliness\"]\n",
    "lit_inds = [\"ai_lit_knowledge\", \"ai_lit_skill\", \"ai_lit_ethics\"]\n",
    "\n",
    "exogenous = {\n",
    "    \"Peer\": \"peer_ai_norms\",\n",
    "    \"Trust\": \"trust_in_ai\",\n",
    "    \"Grade\": \"grade_level_encode\",\n",
    "    \"Income\": \"family_income_quintile\",\n",
    "}\n",
    "\n",
    "# 丢弃含有关键缺失的样本（SEM 对缺失较敏感；也可以切换成 FIML，但 semopy 目前更稳的是完整样本）\n",
    "cols_needed = y_items + usage_inds + strain_inds + lit_inds + list(exogenous.values())\n",
    "dff = df.dropna(subset=cols_needed).copy()\n",
    "\n",
    "\n",
    "def center(s): return s - s.mean()\n",
    "for c in usage_inds + lit_inds:\n",
    "    dff[f\"c_{c}\"] = center(dff[c])\n",
    "\n",
    "# —— 产品指示（为避免过重，取 3 组合；都用“中心化后”的指示）\n",
    "#    UxL 指示： (minutes×knowledge), (days×skill), (iter×ethics)\n",
    "prod_specs = [\n",
    "    (\"c_ai_daily_minutes\", \"c_ai_lit_knowledge\"),\n",
    "    (\"c_ai_use_days_per_week\", \"c_ai_lit_skill\"),\n",
    "    (\"c_iter_per_task\", \"c_ai_lit_ethics\"),\n",
    "]\n",
    "prod_cols = []\n",
    "for a, b in prod_specs:\n",
    "    name = f\"prod_{a[2:]}_x_{b[2:]}\"  # 去掉前缀c_\n",
    "    dff[name] = dff[a] * dff[b]\n",
    "    prod_cols.append(name)\n",
    "\n",
    "print(\"Data ready. N =\", len(dff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Measurement\n",
      "DAI =~ dai_item_1 + dai_item_2 + dai_item_3 + dai_item_4 + dai_item_5 + dai_item_6\n",
      "AI_Usage =~ ai_daily_minutes + ai_use_days_per_week + iter_per_task + prompt_length_avg\n",
      "Strain =~ academic_stress + ucla_loneliness\n",
      "AI_Literacy =~ ai_lit_knowledge + ai_lit_skill + ai_lit_ethics\n",
      "UxL =~ prod_ai_daily_minutes_x_ai_lit_knowledge + prod_ai_use_days_per_week_x_ai_lit_skill + prod_iter_per_task_x_ai_lit_ethics\n",
      "\n",
      "# Structural\n",
      "AI_Usage ~ peer_ai_norms + trust_in_ai\n",
      "DAI ~ AI_Usage + Strain + UxL + peer_ai_norms + trust_in_ai + grade_level_encode + family_income_quintile\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 中文注释：semopy 的模型描述字符串\n",
    "# 说明：\n",
    "# - =~ 表示测量关系（潜变量由哪些指示测量）\n",
    "# - ~  表示结构路径\n",
    "# - 我们使用 UxL 作为潜变量，其指示为前面构造的 3 个产品指标\n",
    "# - 同时放入 Grade、Income 控制 DAI；保留 Peer、Trust → DAI 直接路径做“部分中介”检验（需要可删）\n",
    "\n",
    "model_desc = f\"\"\"\n",
    "# Measurement\n",
    "DAI =~ {' + '.join(y_items)}\n",
    "AI_Usage =~ {' + '.join(usage_inds)}\n",
    "Strain =~ {' + '.join(strain_inds)}\n",
    "AI_Literacy =~ {' + '.join(lit_inds)}\n",
    "UxL =~ {' + '.join(prod_cols)}\n",
    "\n",
    "# Structural\n",
    "AI_Usage ~ {exogenous['Peer']} + {exogenous['Trust']}\n",
    "DAI ~ AI_Usage + Strain + UxL + {exogenous['Peer']} + {exogenous['Trust']} + {exogenous['Grade']} + {exogenous['Income']}\n",
    "\"\"\"\n",
    "\n",
    "print(model_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./outputs/sem/sem_fit_main.xlsx\n",
      "Saved: ./outputs/sem/sem_params_main.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   n_obs     df      logl        AIC         BIC       CFI       TLI  \\\n",
       " 0    860  205.0  1.081913  93.836175  322.168929  0.790602  0.759958   \n",
       " \n",
       "       RMSEA  SRMR  \n",
       " 0  0.064184  None  ,\n",
       "        lval op           rval   Estimate       Std. Err    z-value   p-value\n",
       " 0  AI_Usage  ~  peer_ai_norms   0.459233       1.664402   0.275915  0.782614\n",
       " 1  AI_Usage  ~    trust_in_ai   4.327321       3.229778    1.33982  0.180304\n",
       " 2       DAI  ~       AI_Usage   0.008565       0.000724  11.826264       0.0\n",
       " 3       DAI  ~         Strain   1.645551       0.459953   3.577654  0.000347\n",
       " 4       DAI  ~            UxL -17.780567  119655.168718  -0.000149  0.999881)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Fixed Cell (multi-version safe): Fit SEM main model & export fit indices ===\n",
    "import pandas as pd\n",
    "\n",
    "# --- robust imports for different semopy versions ---\n",
    "from semopy import Model\n",
    "# calc_stats path differs across versions\n",
    "try:\n",
    "    from semopy.model_statistics import calc_stats\n",
    "except Exception:\n",
    "    try:\n",
    "        from semopy.statistics import calc_stats\n",
    "    except Exception:\n",
    "        # some builds expose calc_stats at top-level\n",
    "        from semopy import calc_stats\n",
    "\n",
    "# inspector path also differs\n",
    "try:\n",
    "    from semopy.inspector import inspect as sem_inspect\n",
    "except Exception:\n",
    "    # older versions\n",
    "    from semopy.inspect import inspect as sem_inspect\n",
    "\n",
    "# --- fit model (fit() already does load_dataset + optimize) ---\n",
    "mod = Model(model_desc)\n",
    "_ = mod.fit(dff)\n",
    "\n",
    "# --- normalize fit stats across semopy versions ---\n",
    "def _normalize_fit_stats(stats_obj, n_obs: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    兼容两类返回：\n",
    "    1) pandas.DataFrame（有时是一行多列；有时是行=指标名）\n",
    "    2) 带属性对象（dof, logl, aic, bic, cfi, tli, rmsea, srmr）\n",
    "    输出：单行 DataFrame。\n",
    "    \"\"\"\n",
    "    if isinstance(stats_obj, pd.DataFrame):\n",
    "        df0 = stats_obj.copy()\n",
    "\n",
    "        # 情况A：一行多列\n",
    "        if df0.shape[0] == 1:\n",
    "            row = df0.iloc[0]\n",
    "            def g(keys):\n",
    "                for k in keys:\n",
    "                    if k in row:\n",
    "                        try:\n",
    "                            return float(row[k])\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                return None\n",
    "            out = {\n",
    "                \"n_obs\": n_obs,\n",
    "                \"df\":   g([\"DoF\",\"dof\",\"df\"]),\n",
    "                \"logl\": g([\"LogLik\",\"logl\",\"LogLik.\",\"loglik\"]),\n",
    "                \"AIC\":  g([\"AIC\",\"aic\"]),\n",
    "                \"BIC\":  g([\"BIC\",\"bic\"]),\n",
    "                \"CFI\":  g([\"CFI\",\"cfi\"]),\n",
    "                \"TLI\":  g([\"TLI\",\"tli\",\"NNFI\"]),\n",
    "                \"RMSEA\":g([\"RMSEA\",\"rmsea\"]),\n",
    "                \"SRMR\": g([\"SRMR\",\"srmr\"]),\n",
    "            }\n",
    "            return pd.DataFrame([out])\n",
    "\n",
    "        # 情况B：行=指标名\n",
    "        idx_lower = {str(i).lower(): i for i in df0.index}\n",
    "        def g2(key):\n",
    "            for cand in [key.lower(), key.upper(), key.capitalize()]:\n",
    "                if cand in idx_lower:\n",
    "                    s = df0.loc[idx_lower[cand]]\n",
    "                    try:\n",
    "                        return float(getattr(s, \"values\", [s])[0])\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            return float(s)\n",
    "                        except Exception:\n",
    "                            return None\n",
    "            return None\n",
    "\n",
    "        out = {\n",
    "            \"n_obs\": n_obs,\n",
    "            \"df\":   g2(\"dof\"),\n",
    "            \"logl\": g2(\"loglik\"),\n",
    "            \"AIC\":  g2(\"aic\"),\n",
    "            \"BIC\":  g2(\"bic\"),\n",
    "            \"CFI\":  g2(\"cfi\"),\n",
    "            \"TLI\":  g2(\"tli\"),\n",
    "            \"RMSEA\":g2(\"rmsea\"),\n",
    "            \"SRMR\": g2(\"srmr\"),\n",
    "        }\n",
    "        return pd.DataFrame([out])\n",
    "\n",
    "    # 情况C：对象有属性\n",
    "    out = {\n",
    "        \"n_obs\": n_obs,\n",
    "        \"df\":   getattr(stats_obj, \"dof\", None),\n",
    "        \"logl\": getattr(stats_obj, \"logl\", None),\n",
    "        \"AIC\":  getattr(stats_obj, \"aic\", None),\n",
    "        \"BIC\":  getattr(stats_obj, \"bic\", None),\n",
    "        \"CFI\":  getattr(stats_obj, \"cfi\", None),\n",
    "        \"TLI\":  getattr(stats_obj, \"tli\", None),\n",
    "        \"RMSEA\":getattr(stats_obj, \"rmsea\", None),\n",
    "        \"SRMR\": getattr(stats_obj, \"srmr\", None),\n",
    "    }\n",
    "    return pd.DataFrame([out])\n",
    "\n",
    "# --- compute & export fit indices ---\n",
    "stats_obj = calc_stats(mod)   # 不要传 dff\n",
    "fit_df = _normalize_fit_stats(stats_obj, n_obs=len(dff))\n",
    "fit_path = \"./outputs/sem/sem_fit_main.xlsx\"\n",
    "fit_df.to_excel(fit_path, index=False)\n",
    "print(\"Saved:\", fit_path)\n",
    "\n",
    "# --- export parameter table ---\n",
    "params = sem_inspect(mod)\n",
    "params_path = \"./outputs/sem/sem_params_main.xlsx\"\n",
    "params.to_excel(params_path, index=False)\n",
    "print(\"Saved:\", params_path)\n",
    "\n",
    "fit_df.head(), params.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./outputs/sem/sem_boot_mediation_and_mod.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 中文注释：计算间接效应（基于参数表） + bootstrap CI\n",
    "# 间接：Peer→Usage→DAI / Trust→Usage→DAI\n",
    "# 取系数：Usage~Peer 的回归系数、DAI~AI_Usage 的回归系数，做乘积。\n",
    "# bootstrap：重抽样 N 次，保存乘积分布。\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "def get_coef(df_params, lhs, op, rhs):\n",
    "    # 在 semopy 参数表中筛选某个路径的系数（估计值）\n",
    "    m = df_params\n",
    "    row = m[(m[\"lval\"]==lhs) & (m[\"op\"]==op) & (m[\"rval\"]==rhs)]\n",
    "    return float(row[\"Estimate\"].values[0])\n",
    "\n",
    "# 当前样本的点估计\n",
    "b_peer_usage = get_coef(params, \"AI_Usage\", \"~\", exogenous[\"Peer\"])\n",
    "b_trust_usage = get_coef(params, \"AI_Usage\", \"~\", exogenous[\"Trust\"])\n",
    "b_usage_dai  = get_coef(params, \"DAI\", \"~\", \"AI_Usage\")\n",
    "b_uxl_dai    = get_coef(params, \"DAI\", \"~\", \"UxL\")\n",
    "\n",
    "ind_peer = b_peer_usage * b_usage_dai\n",
    "ind_trust = b_trust_usage * b_usage_dai\n",
    "\n",
    "# Bootstrap\n",
    "B = 1000\n",
    "boot_records = []\n",
    "idx_all = np.arange(len(dff))\n",
    "for _ in trange(B, leave=False):\n",
    "    boot_idx = np.random.choice(idx_all, size=len(idx_all), replace=True)\n",
    "    d_boot = dff.iloc[boot_idx].reset_index(drop=True)\n",
    "    m_boot = Model(model_desc)\n",
    "    try:\n",
    "        Optimizer(m_boot).optimize(d_boot)\n",
    "        p_boot = sem_inspect(m_boot)\n",
    "\n",
    "        bu = get_coef(p_boot, \"DAI\", \"~\", \"AI_Usage\")\n",
    "        bpu = get_coef(p_boot, \"AI_Usage\", \"~\", exogenous[\"Peer\"])\n",
    "        btu = get_coef(p_boot, \"AI_Usage\", \"~\", exogenous[\"Trust\"])\n",
    "        buxl = get_coef(p_boot, \"DAI\", \"~\", \"UxL\")\n",
    "\n",
    "        boot_records.append({\n",
    "            \"ind_peer\": bpu*bu,\n",
    "            \"ind_trust\": btu*bu,\n",
    "            \"b_UxL_to_DAI\": buxl\n",
    "        })\n",
    "    except Exception:\n",
    "        # 个别重抽失败就跳过\n",
    "        continue\n",
    "\n",
    "boot_df = pd.DataFrame(boot_records)\n",
    "def ci(s): \n",
    "    return pd.Series({\"mean\": s.mean(), \"2.5%\": s.quantile(0.025), \"97.5%\": s.quantile(0.975)})\n",
    "ci_df = boot_df.apply(ci).T\n",
    "ci_df_path = \"./outputs/sem/sem_boot_mediation_and_mod.xlsx\"\n",
    "ci_df.to_excel(ci_df_path)\n",
    "print(\"Saved:\", ci_df_path)\n",
    "\n",
    "ci_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./outputs/sem/sem_model_graph.png\n"
     ]
    }
   ],
   "source": [
    "# 中文注释：输出 SEM 结构图（.png）\n",
    "fig_file =  \"./outputs/sem/sem_model_graph.png\"\n",
    "semplot(mod, fig_file)   # 需要本机安装 graphviz\n",
    "print(\"Saved:\", fig_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异质性分析：按照学校类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./outputs/sem/sem_groups_school_type.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>lhs</th>\n",
       "      <th>op</th>\n",
       "      <th>rhs</th>\n",
       "      <th>coef</th>\n",
       "      <th>CFI</th>\n",
       "      <th>TLI</th>\n",
       "      <th>RMSEA</th>\n",
       "      <th>SRMR</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>school_0</td>\n",
       "      <td>AI_Usage</td>\n",
       "      <td>~</td>\n",
       "      <td>peer_ai_norms</td>\n",
       "      <td>-0.223838</td>\n",
       "      <td>0.665335</td>\n",
       "      <td>0.616359</td>\n",
       "      <td>0.094993</td>\n",
       "      <td>None</td>\n",
       "      <td>89.679015</td>\n",
       "      <td>235.76342</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>school_0</td>\n",
       "      <td>AI_Usage</td>\n",
       "      <td>~</td>\n",
       "      <td>trust_in_ai</td>\n",
       "      <td>0.227389</td>\n",
       "      <td>0.665335</td>\n",
       "      <td>0.616359</td>\n",
       "      <td>0.094993</td>\n",
       "      <td>None</td>\n",
       "      <td>89.679015</td>\n",
       "      <td>235.76342</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>school_0</td>\n",
       "      <td>DAI</td>\n",
       "      <td>~</td>\n",
       "      <td>AI_Usage</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.665335</td>\n",
       "      <td>0.616359</td>\n",
       "      <td>0.094993</td>\n",
       "      <td>None</td>\n",
       "      <td>89.679015</td>\n",
       "      <td>235.76342</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>school_0</td>\n",
       "      <td>DAI</td>\n",
       "      <td>~</td>\n",
       "      <td>Strain</td>\n",
       "      <td>-882.099793</td>\n",
       "      <td>0.665335</td>\n",
       "      <td>0.616359</td>\n",
       "      <td>0.094993</td>\n",
       "      <td>None</td>\n",
       "      <td>89.679015</td>\n",
       "      <td>235.76342</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>school_0</td>\n",
       "      <td>DAI</td>\n",
       "      <td>~</td>\n",
       "      <td>UxL</td>\n",
       "      <td>2.269699</td>\n",
       "      <td>0.665335</td>\n",
       "      <td>0.616359</td>\n",
       "      <td>0.094993</td>\n",
       "      <td>None</td>\n",
       "      <td>89.679015</td>\n",
       "      <td>235.76342</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group       lhs op            rhs        coef       CFI       TLI  \\\n",
       "0  school_0  AI_Usage  ~  peer_ai_norms   -0.223838  0.665335  0.616359   \n",
       "1  school_0  AI_Usage  ~    trust_in_ai    0.227389  0.665335  0.616359   \n",
       "2  school_0       DAI  ~       AI_Usage    0.010162  0.665335  0.616359   \n",
       "3  school_0       DAI  ~         Strain -882.099793  0.665335  0.616359   \n",
       "4  school_0       DAI  ~            UxL    2.269699  0.665335  0.616359   \n",
       "\n",
       "      RMSEA  SRMR        AIC        BIC    n  \n",
       "0  0.094993  None  89.679015  235.76342  155  \n",
       "1  0.094993  None  89.679015  235.76342  155  \n",
       "2  0.094993  None  89.679015  235.76342  155  \n",
       "3  0.094993  None  89.679015  235.76342  155  \n",
       "4  0.094993  None  89.679015  235.76342  155  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Fixed Cell: Groupwise SEM fits by school_type_encode with robust stats export ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from semopy import Model\n",
    "\n",
    "# 复用你前面那格里已定义的工具：\n",
    "# - sem_inspect  : 已做多版本兼容的 inspect()\n",
    "# - calc_stats   : 已做多版本兼容的 calc_stats()\n",
    "# - _normalize_fit_stats(stats_obj, n_obs): 将 fit 指标归一为单行 DataFrame\n",
    "\n",
    "group_col = \"school_type_encode\"\n",
    "assert group_col in dff.columns, \"缺少 school_type_encode 列\"\n",
    "\n",
    "# --- 兼容：从 exogenous 字典里取命名；若不存在则直接用传入值 ---\n",
    "def _rhs_name(name: str) -> str:\n",
    "    try:\n",
    "        return exogenous.get(name, name)\n",
    "    except NameError:\n",
    "        return name\n",
    "\n",
    "# 关键路径（左右变量名称与主模型一致）\n",
    "key_paths = [\n",
    "    (\"AI_Usage\", \"~\", _rhs_name(\"Peer\")),\n",
    "    (\"AI_Usage\", \"~\", _rhs_name(\"Trust\")),\n",
    "    (\"DAI\", \"~\", \"AI_Usage\"),\n",
    "    (\"DAI\", \"~\", \"Strain\"),\n",
    "    (\"DAI\", \"~\", \"UxL\"),\n",
    "    (\"DAI\", \"~\", _rhs_name(\"Grade\")),\n",
    "    (\"DAI\", \"~\", _rhs_name(\"Income\")),\n",
    "]\n",
    "\n",
    "# --- 兼容不同 semopy 版本的参数表取系数 ---\n",
    "def get_coef(params_df: pd.DataFrame, lhs: str, op: str, rhs: str) -> float:\n",
    "    if params_df is None or len(params_df) == 0:\n",
    "        return np.nan\n",
    "    df = params_df.copy()\n",
    "    # 统一列名到小写，便于匹配\n",
    "    df.columns = [str(c).lower() for c in df.columns]\n",
    "\n",
    "    # 候选列名映射\n",
    "    lhs_cols = [\"lhs\", \"lval\", \"lvar\", \"left\"]\n",
    "    rhs_cols = [\"rhs\", \"rval\", \"rvar\", \"right\"]\n",
    "    op_cols  = [\"op\", \"operator\"]\n",
    "    est_cols = [\"est\", \"estimate\", \"value\", \"beta\", \"coef\"]\n",
    "\n",
    "    def _first_col(cands):\n",
    "        for c in cands:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    lcol = _first_col(lhs_cols)\n",
    "    rcol = _first_col(rhs_cols)\n",
    "    ocol = _first_col(op_cols)\n",
    "    ecol = _first_col(est_cols)\n",
    "\n",
    "    if lcol is None or rcol is None or ocol is None or ecol is None:\n",
    "        return np.nan\n",
    "\n",
    "    sub = df[(df[lcol] == lhs) & (df[ocol] == op) & (df[rcol] == rhs)]\n",
    "    if sub.empty:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(sub.iloc[0][ecol])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# --- 分组拟合并汇总 ---\n",
    "rows = []\n",
    "for g, dg in dff.groupby(group_col):\n",
    "    # 拟合（fit 内部已 load_dataset + optimize）\n",
    "    mg = Model(model_desc)\n",
    "    _ = mg.fit(dg)\n",
    "\n",
    "    # 参数与拟合度\n",
    "    pg = sem_inspect(mg)            # 参数表（含路径/载荷/显著性）\n",
    "    stats_obj = calc_stats(mg)      # 不要传 dg\n",
    "    fit_row = _normalize_fit_stats(stats_obj, n_obs=len(dg)).iloc[0].to_dict()\n",
    "\n",
    "    # 记录每个关键路径的系数 + 分组拟合指标\n",
    "    for (lhs, op, rhs) in key_paths:\n",
    "        rows.append({\n",
    "            \"group\": f\"school_{g}\",\n",
    "            \"lhs\": lhs, \"op\": op, \"rhs\": rhs,\n",
    "            \"coef\": get_coef(pg, lhs, op, rhs),\n",
    "            \"CFI\": fit_row.get(\"CFI\"),\n",
    "            \"TLI\": fit_row.get(\"TLI\"),\n",
    "            \"RMSEA\": fit_row.get(\"RMSEA\"),\n",
    "            \"SRMR\": fit_row.get(\"SRMR\"),\n",
    "            \"AIC\": fit_row.get(\"AIC\"),\n",
    "            \"BIC\": fit_row.get(\"BIC\"),\n",
    "            \"n\": int(fit_row.get(\"n_obs\", len(dg)))\n",
    "        })\n",
    "\n",
    "school_cmp = pd.DataFrame(rows)\n",
    "school_cmp_path =  \"./outputs/sem/sem_groups_school_type.xlsx\"\n",
    "school_cmp.to_excel(school_cmp_path, index=False)\n",
    "print(\"Saved:\", school_cmp_path)\n",
    "\n",
    "school_cmp.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多组异质性：按 Major Group（高/中/低）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./outputs/sem/sem_groups_major.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>lhs</th>\n",
       "      <th>op</th>\n",
       "      <th>rhs</th>\n",
       "      <th>coef</th>\n",
       "      <th>CFI</th>\n",
       "      <th>TLI</th>\n",
       "      <th>RMSEA</th>\n",
       "      <th>SRMR</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>major_high</td>\n",
       "      <td>AI_Usage</td>\n",
       "      <td>~</td>\n",
       "      <td>peer_ai_norms</td>\n",
       "      <td>6.622748</td>\n",
       "      <td>0.814551</td>\n",
       "      <td>0.787412</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>None</td>\n",
       "      <td>93.068276</td>\n",
       "      <td>272.423747</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>major_high</td>\n",
       "      <td>AI_Usage</td>\n",
       "      <td>~</td>\n",
       "      <td>trust_in_ai</td>\n",
       "      <td>10.290709</td>\n",
       "      <td>0.814551</td>\n",
       "      <td>0.787412</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>None</td>\n",
       "      <td>93.068276</td>\n",
       "      <td>272.423747</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>major_high</td>\n",
       "      <td>DAI</td>\n",
       "      <td>~</td>\n",
       "      <td>AI_Usage</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.814551</td>\n",
       "      <td>0.787412</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>None</td>\n",
       "      <td>93.068276</td>\n",
       "      <td>272.423747</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>major_high</td>\n",
       "      <td>DAI</td>\n",
       "      <td>~</td>\n",
       "      <td>Strain</td>\n",
       "      <td>-241.380596</td>\n",
       "      <td>0.814551</td>\n",
       "      <td>0.787412</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>None</td>\n",
       "      <td>93.068276</td>\n",
       "      <td>272.423747</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>major_high</td>\n",
       "      <td>DAI</td>\n",
       "      <td>~</td>\n",
       "      <td>UxL</td>\n",
       "      <td>-10.208824</td>\n",
       "      <td>0.814551</td>\n",
       "      <td>0.787412</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>None</td>\n",
       "      <td>93.068276</td>\n",
       "      <td>272.423747</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        group       lhs op            rhs        coef       CFI       TLI  \\\n",
       "0  major_high  AI_Usage  ~  peer_ai_norms    6.622748  0.814551  0.787412   \n",
       "1  major_high  AI_Usage  ~    trust_in_ai   10.290709  0.814551  0.787412   \n",
       "2  major_high       DAI  ~       AI_Usage    0.008254  0.814551  0.787412   \n",
       "3  major_high       DAI  ~         Strain -241.380596  0.814551  0.787412   \n",
       "4  major_high       DAI  ~            UxL  -10.208824  0.814551  0.787412   \n",
       "\n",
       "      RMSEA  SRMR        AIC         BIC    n  \n",
       "0  0.062749  None  93.068276  272.423747  310  \n",
       "1  0.062749  None  93.068276  272.423747  310  \n",
       "2  0.062749  None  93.068276  272.423747  310  \n",
       "3  0.062749  None  93.068276  272.423747  310  \n",
       "4  0.062749  None  93.068276  272.423747  310  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Fixed Cell: Groupwise SEM fits by major_group (high/mid/low) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from semopy import Model\n",
    "\n",
    "assert \"major_group\" in dff.columns, \"缺少 major_group 列\"\n",
    "\n",
    "# 允许的组次序（如数据不存在也会跳过，且打印提示）\n",
    "ordered_groups = [\"high\", \"mid\", \"low\"]\n",
    "\n",
    "rows = []\n",
    "for g in ordered_groups:\n",
    "    dg = dff[dff[\"major_group\"] == g]\n",
    "    if len(dg) == 0:\n",
    "        print(f\"[Skip] major_group='{g}' 无样本。\")\n",
    "        continue\n",
    "    if len(dg) < 50:\n",
    "        print(f\"[Warn] major_group='{g}' 样本量较小（n={len(dg)}），结果可能不稳定。\")\n",
    "\n",
    "    # 拟合（fit 内部已 load_dataset + optimize）\n",
    "    mg = Model(model_desc)\n",
    "    _ = mg.fit(dg)\n",
    "\n",
    "    # 参数与拟合度（多版本兼容）\n",
    "    pg = sem_inspect(mg)           # 参数表（含路径/载荷/显著性）\n",
    "    stats_obj = calc_stats(mg)     # 新版不需要传 dg\n",
    "    fit_row = _normalize_fit_stats(stats_obj, n_obs=len(dg)).iloc[0].to_dict()\n",
    "\n",
    "    # 汇总关键路径\n",
    "    for (lhs, op, rhs) in key_paths:\n",
    "        rows.append({\n",
    "            \"group\": f\"major_{g}\",\n",
    "            \"lhs\": lhs, \"op\": op, \"rhs\": rhs,\n",
    "            \"coef\": get_coef(pg, lhs, op, rhs),\n",
    "            \"CFI\": fit_row.get(\"CFI\"),\n",
    "            \"TLI\": fit_row.get(\"TLI\"),\n",
    "            \"RMSEA\": fit_row.get(\"RMSEA\"),\n",
    "            \"SRMR\": fit_row.get(\"SRMR\"),\n",
    "            \"AIC\": fit_row.get(\"AIC\"),\n",
    "            \"BIC\": fit_row.get(\"BIC\"),\n",
    "            \"n\": int(fit_row.get(\"n_obs\", len(dg)))\n",
    "        })\n",
    "\n",
    "major_cmp = pd.DataFrame(rows)\n",
    "major_cmp_path =  \"./outputs/sem/sem_groups_major.xlsx\"\n",
    "major_cmp.to_excel(major_cmp_path, index=False)\n",
    "print(\"Saved:\", major_cmp_path)\n",
    "\n",
    "major_cmp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./outputs/sem/sem_paths_main_structural.xlsx\n",
      "Saved: ./outputs/sem/sem_loadings.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === Fixed Cell: Export structural paths & loadings (robust across semopy versions) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _pick_col(df, candidates, required=True):\n",
    "    \"\"\"从多个候选名里找第一列；required=False 时可返回 None。\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"None of columns found: {candidates}\")\n",
    "    return None\n",
    "\n",
    "# 1) 结构路径 (op == \"~\")\n",
    "dfp = params.copy()\n",
    "\n",
    "# 列名兼容：常见有 lval/rval/op/Estimate/SE/z-value/p-value 或 Est/Std.Err/P(>|z|)\n",
    "lhs_col = _pick_col(dfp, [\"lval\", \"lhs\"])\n",
    "rhs_col = _pick_col(dfp, [\"rval\", \"rhs\"])\n",
    "op_col  = _pick_col(dfp, [\"op\"])\n",
    "\n",
    "est_col = _pick_col(dfp, [\"Estimate\", \"Est\", \"est\", \"Std.Estimate\"], required=True)\n",
    "se_col  = _pick_col(dfp, [\"SE\", \"Std.Err\", \"StdErr\", \"SE (Robust)\", \"SE (robust)\"], required=False)\n",
    "z_col   = _pick_col(dfp, [\"z-value\", \"z\", \"Z\"], required=False)\n",
    "p_col   = _pick_col(dfp, [\"p-value\", \"P-value\", \"pvalue\", \"P(>|z|)\", \"p\"], required=False)\n",
    "\n",
    "# 仅取结构路径\n",
    "ps = dfp[dfp[op_col] == \"~\"].copy()\n",
    "\n",
    "# 转数值（字符串也能转成数值；失败给 NaN）\n",
    "ps[\"_est\"] = pd.to_numeric(ps[est_col], errors=\"coerce\")\n",
    "if se_col is not None:\n",
    "    ps[\"_se\"]  = pd.to_numeric(ps[se_col], errors=\"coerce\")\n",
    "else:\n",
    "    ps[\"_se\"]  = np.nan\n",
    "if z_col is not None:\n",
    "    ps[\"_z\"]   = pd.to_numeric(ps[z_col], errors=\"coerce\")\n",
    "else:\n",
    "    ps[\"_z\"]   = np.nan\n",
    "if p_col is not None:\n",
    "    ps[\"_p\"]   = pd.to_numeric(ps[p_col], errors=\"coerce\")\n",
    "else:\n",
    "    ps[\"_p\"]   = np.nan\n",
    "\n",
    "# 若 SE 缺失且 z 有值，用 SE = Estimate / z 回推（z=0 或缺失则仍为 NaN）\n",
    "mask_se_missing = ps[\"_se\"].isna() & ps[\"_z\"].notna() & (ps[\"_z\"] != 0)\n",
    "ps.loc[mask_se_missing, \"_se\"] = ps.loc[mask_se_missing, \"_est\"] / ps.loc[mask_se_missing, \"_z\"]\n",
    "\n",
    "# 显著性星号\n",
    "def _stars(p):\n",
    "    if pd.isna(p): return \"\"\n",
    "    return \"***\" if p < 0.001 else (\"**\" if p < 0.01 else (\"*\" if p < 0.05 else \"\"))\n",
    "\n",
    "param_struct = ps[[lhs_col, op_col, rhs_col, \"_est\", \"_se\", \"_z\", \"_p\"]].rename(columns={\n",
    "    lhs_col: \"lhs\", op_col: \"op\", rhs_col: \"rhs\",\n",
    "    \"_est\": \"beta\", \"_se\": \"se\", \"_z\": \"z\", \"_p\": \"p\"\n",
    "})\n",
    "\n",
    "param_struct[\"sig\"] = param_struct[\"p\"].apply(_stars)\n",
    "\n",
    "# 导出\n",
    "struct_path = \"./outputs/sem/sem_paths_main_structural.xlsx\"\n",
    "param_struct.to_excel(struct_path, index=False)\n",
    "print(\"Saved:\", struct_path)\n",
    "\n",
    "# 2) 测量载荷 (op == \"=~\")\n",
    "pl = dfp[dfp[op_col] == \"=~\"].copy()\n",
    "\n",
    "# 同样做数值化；不同版本有标准化载荷列(如 Std.Estimate)\n",
    "pl[\"_est\"] = pd.to_numeric(pl[est_col], errors=\"coerce\")\n",
    "\n",
    "loadings = pl[[lhs_col, op_col, rhs_col, \"_est\"]].rename(columns={\n",
    "    lhs_col: \"latent\", op_col: \"op\", rhs_col: \"indicator\", \"_est\": \"loading\"\n",
    "})\n",
    "\n",
    "load_path =  \"./outputs/sem/sem_loadings.xlsx\"\n",
    "loadings.to_excel(load_path, index=False)\n",
    "print(\"Saved:\", load_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
